{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a55499",
   "metadata": {},
   "source": [
    "# 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b60aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset,TensorDataset\n",
    "from torch.nn import functional as F\n",
    "import re\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from konlpy.tag import Mecab\n",
    "import wandb \n",
    "import collections\n",
    "import pickle\n",
    "import kss\n",
    "from hanspell import spell_checker\n",
    "\n",
    "from ktextaug import TextAugmentation\n",
    "import ktextaug\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelForMaskedLM\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.family'] = 'NanumGothic'\n",
    "\n",
    "random_seed = 42 \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(random_seed)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d63db",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ddd13",
   "metadata": {},
   "source": [
    "# 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "677c03f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):#파일이름만 넣기\n",
    "    data_path = os.getenv('HOME') + '/aiffel/project/FnGuide/data/ad_news'#경로는 자신의 경로로 바꿔서 하면 됨\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    df = pd.read_csv(file_path, encoding='utf-8', delimiter='\\t')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5debdb",
   "metadata": {},
   "source": [
    "## 1-1 사전학습 모델 및 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4aeeed71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"klue/roberta-base\"  # 허깅페이스에서 모델만 갈아끼우면 됨\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 예측하고자 하는 클래스의 수를 넣어줘야함\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4d4ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /aiffel/aiffel/project/FnGuide/kfdeberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# kb-albert-char-base-v2, kfdeberta-base\n",
    "model_name='kfdeberta-base'\n",
    "\n",
    "p_model_path = os.getenv('HOME') + \"/aiffel/project/FnGuide/kfdeberta-base\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(p_model_path, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(p_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16488c35",
   "metadata": {},
   "source": [
    "## 1-2 모델 이진분류로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6032a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73308231",
   "metadata": {},
   "source": [
    "### 1-2-1. amphora/KorFinASC-XLM-RoBERTa용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "69f1314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 크기 변경\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=1024, out_features=2, bias=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9359e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#재확인\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11418022",
   "metadata": {},
   "source": [
    "# 3. 데이터 전처리\n",
    "## 3-1. 컬럼 삭제 및 레이블 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42003e",
   "metadata": {},
   "source": [
    "불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c56ade7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [] \n",
    "stop_path = os.getenv('HOME') + '/aiffel/project/FnGuide/stopwords.txt'  # 자기에게 맞는 경로 설정\n",
    "\n",
    "with open(stop_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.read()\n",
    "\n",
    "    sentences = re.findall(r'\"([^\"]*)\"', lines)  # 작은따옴표로 둘러싸인 내용 추출\n",
    "    for sentence in sentences:\n",
    "        stopwords.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e7c3ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(sentence, stopwords=None):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'\\([^)]*\\)', r'', sentence) #괄호로 둘러싸인 부분 제거\n",
    "    sentence = re.sub(r'#\\w+', '', sentence) #해쉬태그 문자 제거\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) #문장 내의 구두점을 공백과 함께 분리(숫자 없앰 -> 금융이라 중요하다 생각)\n",
    "    sentence = re.sub(r'[^a-zA-Z가-힣0-9?.!,% ]+', r' ', sentence) #영문 알파벳, 한글, 숫자, 구두점을 제외한 모든 문자를 제거\n",
    "    sentence = re.sub(r\"['\\n']+\", r\"\", sentence) #개행 문자 제거\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #연속된 공백을 하나의 공백으로 변환\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if stopwords:\n",
    "        words = sentence.split()\n",
    "        filtered_words = [word for word in words if word not in stopwords]\n",
    "        sentence = ' '.join(filtered_words)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c723df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, data_type, stopwords=None):\n",
    "    df = df[['document', 'label']]\n",
    "    \n",
    "    if data_type=='train':\n",
    "        df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "#################표준어 체크\n",
    "\n",
    "    new_sentences = []\n",
    "\n",
    "    for sentence in tqdm(df['document']):\n",
    "        sentence = transform(sentence, stopwords)\n",
    "        try:\n",
    "            sentence = sentence.replace('\\\\', '')\n",
    "            spelled_sent = spell_checker.check(sentence)\n",
    "            new_sentence = spelled_sent.checked\n",
    "            \n",
    "            if not new_sentence:\n",
    "                new_sentence = sentence\n",
    "                \n",
    "            new_sentences.append(new_sentence)\n",
    "                \n",
    "        except:\n",
    "            \n",
    "            new_sentences.append(sentence)\n",
    "\n",
    "    df['document'] = new_sentences\n",
    "    \n",
    "    if data_type == 'train':\n",
    "        df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "95d24a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data('modified_train.csv')\n",
    "val_df = load_data('ratings_val.csv')\n",
    "test_df = load_data('ratings_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3335f3fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5412/5412 [08:00<00:00, 11.26it/s]\n",
      "100%|██████████| 679/679 [00:59<00:00, 11.37it/s]\n",
      "/tmp/ipykernel_31/4293808085.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['document'] = new_sentences\n",
      "100%|██████████| 679/679 [00:57<00:00, 11.72it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = preprocess_data(train_df, data_type='train', stopwords=stopwords)\n",
    "val_df = preprocess_data(val_df, data_type='val', stopwords=stopwords)\n",
    "test_df = preprocess_data(test_df, data_type='test', stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc24117",
   "metadata": {},
   "source": [
    "### 3-1-1. Back translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4e577c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate(data: pd.DataFrame, label: int, num: int, mode: str=\"back_translate\", target_language: str='en', prob: int=0.4)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            data (pandas.DataFrame) : CSV 파일로 구성된 데이터\n",
    "            label (int) : 증강에 사용할 레이블\n",
    "            num (int) : 증강 개수\n",
    "            mode (str, optional) : 데이터 증강 모드. (back_translate/noise_add), 기본값은 \"back_translate\"\n",
    "            target_language (str, optional) : 번역할 언어. 기본값은 'en' (영어)\n",
    "            prob (float, optional) : noise_add 모드에서의 적용 확률\n",
    "        \n",
    "        Returns:\n",
    "            augmented_df (pd.DataFrame) : 증강된 데이터들로 구성된 DataFrame  \n",
    "    \"\"\"\n",
    "    \n",
    "    # TextAugmentation 객체 생성\n",
    "    agent = TextAugmentation(tokenizer=\"mecab\", num_processes=1)\n",
    "\n",
    "    # 선택한 레이블로 데이터 필터링\n",
    "    filtered_data = data[data['label'] == label]['document'].tolist()\n",
    "\n",
    "    # 데이터 증강\n",
    "    augmented_data = []\n",
    "    selected_data_set = set()  # 중복 선택된 데이터를 추적하기 위한 집합\n",
    "\n",
    "    for _ in range(num):\n",
    "        selected_data = random.choice(filtered_data)  # 원본 데이터 중에서 무작위로 선택\n",
    "\n",
    "        while selected_data in selected_data_set:\n",
    "            selected_data = random.choice(filtered_data)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "        selected_data_set.add(selected_data)  # 선택된 데이터를 집합에 추가\n",
    "\n",
    "        # 데이터 증강\n",
    "        if mode == \"noise_add\":\n",
    "            generated_data = agent.generate(selected_data, mode=\"noise_add\", prob=prob, noise_mode=['phonological_change', 'vowel_change', 'jamo_split'])\n",
    "        else:\n",
    "            generated_data = agent.generate(selected_data, mode=\"back_translate\", target_language=target_language)  #일본어로 하려면 target_language='jp'\n",
    "        \n",
    "        augmented_data.append(generated_data)\n",
    "\n",
    "    # 생성된 데이터를 DataFrame으로 변환\n",
    "    augmented_df = pd.DataFrame({'document': augmented_data, 'label': label})\n",
    "    \n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2005f0",
   "metadata": {},
   "source": [
    "### 3-1-2. Document Mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4e6f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_blocks = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d178f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text: str, mode: str ='kss') -> list :\n",
    "    \"\"\"\n",
    "    문장을 분리하는 함수\n",
    "    \n",
    "    Input:\n",
    "        text (str): 증강할 문장\n",
    "        mode (str): 문장을 분리하는 방식\n",
    "\n",
    "    Output:\n",
    "        sentences(list): 분리된 문장 리스트\n",
    "    \"\"\"\n",
    "    if mode == 'to':\n",
    "        sentences = tokenizer.tokenize(text)  # 토크나이저를 사용하여 문장 단위로 분리\n",
    "\n",
    "    else:\n",
    "        sentences = kss.split_sentences(text)  # Split sentences using kss library\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86bd2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_document(data, label, num, min_blocks = min_blocks, mix_ratio_1: int=0.3, mix_ratio_2: int=0.3, mode='kss')->pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            data (pandas.DataFrame) : CSV 파일로 구성된 데이터\n",
    "            label (int) : 증강에 사용할 레이블\n",
    "            num (int) : 증강 개수\n",
    "            min_blocks (str): 최소 문장 수\n",
    "            mix_ratio_1 (int) : 첫번째 문장에 제거할 비율\n",
    "            mix_ratio_2 (int) : 두번쨰 문장에서 삽입할 비율\n",
    "            mode (str, optional) : 데이터 증강 모드. (kss/tokenize), 기본값은 \"back_translate\"\n",
    "            target_language (str, optional) : 번역할 언어. 기본값은 'en' (영어)\n",
    "            prob (float, optional) : noise_add 모드에서의 적용 확률\n",
    "        \n",
    "        Returns:\n",
    "            augmented_df (pd.DataFrame) : 증강된 데이터들로 구성된 DataFrame  \n",
    "    \"\"\"\n",
    "    augmented_data = []\n",
    "    filtered_data = data[data['label'] == label]['document'].tolist()\n",
    "    \n",
    "    selected_data_set = set()  # 중복 선택된 쌍을 추적하기 위한 집합\n",
    "\n",
    "    for _ in range(num):\n",
    "        selected_data = random.sample(filtered_data, 2)  # 원본 데이터 중에서 2개를 무작위로 선택\n",
    "\n",
    "        while tuple(selected_data) in selected_data_set:\n",
    "            selected_data = random.sample(filtered_data, 2)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "        selected_data_set.add(tuple(selected_data))\n",
    "\n",
    "        sentences_1 = tokenize_sentences(selected_data[0], mode)  # 첫 번째 선택된 데이터의 문장으로 분리\n",
    "        sentences_2 = tokenize_sentences(selected_data[1], mode)  # 두 번째 선택된 데이터의 문장으로 분리\n",
    "\n",
    "        # sentences_1에서 특정 비율만큼 문장 제거\n",
    "        num_sentences_to_remove = int(len(sentences_1) * mix_ratio_1)\n",
    "        removed_sentences = random.sample(sentences_1, num_sentences_to_remove)\n",
    "        augmented_sentences_1 = [sentence for sentence in sentences_1 if sentence not in removed_sentences]\n",
    "\n",
    "        \n",
    "        # sentences_2에서 특정 비율만큼 문장 추출\n",
    "        num_sentences_to_extract_2 = int(len(sentences_2) * mix_ratio_2)\n",
    "        extracted_sentences_2 = random.sample(sentences_2, num_sentences_to_extract_2)\n",
    "        \n",
    "        # 추출된 문장들을 랜덤하게 위치에 삽입하여 augmented_sentences_1에 삽입\n",
    "        for sentence in extracted_sentences_2:\n",
    "            random_index = random.randint(0, len(augmented_sentences_1))\n",
    "            augmented_sentences_1.insert(random_index, sentence)\n",
    "        \n",
    "        if mode == 'to':\n",
    "            augmented_text = tokenizer.convert_tokens_to_string(augmented_sentences_1)  # 토큰을 다시 문자열로 변환하여 문서로 만듦\n",
    "        else:\n",
    "            augmented_text = ' '.join(augmented_sentences_1)  # 문장 내의 단어들을 공백으로 연결하여 문장으로 만듦\n",
    "\n",
    "        augmented_data.append(augmented_text)\n",
    "        \n",
    "    # 생성된 증강 데이터에 레이블 1 추가\n",
    "    augmented_df = pd.DataFrame({'document': augmented_data, 'label': 1})\n",
    "    \n",
    "    # 원래 데이터에 추가\n",
    "#     df = pd.concat([data, augmented_df], ignore_index=True)\n",
    "    \n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c349ab8",
   "metadata": {},
   "source": [
    "### 3-1-3. KoEDA\n",
    "\n",
    "p = (alpha_sr, alpha_ri, alpha_rs, prob_rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2a2b4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from koeda import EDA\n",
    "\n",
    "def ag_koeda(data: pd.DataFrame, label: int, num: int, p: tuple=(0.3, 0.3, 0.3, 0.3))-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        data (DataFrame): 증강시킬 데이터가 있는 데이터 프레임\n",
    "        label (int): 증강시키고 싶은 label\n",
    "        num (int): 증강시킬 데이터의 수\n",
    "        p (tuple): alpha_sr, alpha_ri, alpha_rs, prob_rd 기법의 수치\n",
    "        \n",
    "    Output:\n",
    "        augmented_df (DataFrame): 증강시킨 데이터가 있는 데이터 프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    eda = EDA(morpheme_analyzer=\"Mecab\", alpha_sr=p[0], alpha_ri=p[1], alpha_rs=p[2], prob_rd=p[3])\n",
    "\n",
    "    # 데이터 필터링\n",
    "    filtered_data = data[data['label'] == label]['document'].tolist()\n",
    "    \n",
    "    selected_data_set = set()  # 중복 선택된 데이터를 추적하기 위한 집합\n",
    "\n",
    "    # 데이터 증강\n",
    "    augmented_data = []\n",
    "    for _ in range(num):\n",
    "        selected_data = random.choice(filtered_data)  # 원본 데이터 중에서 무작위로 선택\n",
    "\n",
    "        while selected_data in selected_data_set:\n",
    "            selected_data = random.choice(filtered_data)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "        selected_data_set.add(selected_data)  # 선택된 데이터를 집합에 추가\n",
    "\n",
    "        # 데이터 증강\n",
    "        generated_data = eda(selected_data, p=p, repetition=1)  # repetition: 한 문장을 몇 개로 증강시킬 것인지\n",
    "        augmented_data.append(generated_data)\n",
    "\n",
    "    # 생성된 데이터를 DataFrame으로 변환\n",
    "    augmented_df = pd.DataFrame({'document': augmented_data, 'label': label})\n",
    "\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091d6d76",
   "metadata": {},
   "source": [
    "### 3-1-4. Fill Masked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a12a71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "fill_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3595ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_masked_augmentation(df, label_num, random_number, num_masks_ratio, max_length=512):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        df (DataFrame): 증강시킬 데이터가 있는 데이터 프레임\n",
    "        label_num (int): 증강시키고 싶은 label number\n",
    "        random_number (int): 증강시킬 데이터의 수\n",
    "        num_masks_ratio (float): 본문에서 마스킹 시킬 비율\n",
    "        max_length (int): sentence 토큰의 최대 길이\n",
    "        \n",
    "    Output:\n",
    "        aug_data (DataFrame): 증강시킨 데이터가 있는 데이터 프레임\n",
    "    \"\"\"\n",
    "    label_data = df[df['label'] == label_num]\n",
    "    random_sentences = random.sample(label_data['document'].tolist(), random_number)\n",
    "    predicted_tokens_list = []\n",
    "    fill_model.to(device)\n",
    "\n",
    "    for sentence in tqdm(random_sentences):\n",
    "        num_masks = int(len(sentence.split()) * num_masks_ratio)\n",
    "        words = sentence.split()\n",
    "        masked_indices = random.sample(range(len(words)), num_masks)\n",
    "        masked_text = ' '.join('[MASK]' if i in masked_indices else word for i, word in enumerate(words))\n",
    "        tokenized_sentence = fill_tokenizer.tokenize(masked_text)\n",
    "        masked_indices = [i for i, token in enumerate(tokenized_sentence) if token == fill_tokenizer.mask_token]\n",
    "\n",
    "        # 문장 길이 체크 및 잘라내기\n",
    "        if len(tokenized_sentence) > max_length:\n",
    "            continue\n",
    "\n",
    "        predicted_tokens = []\n",
    "        for index in masked_indices:\n",
    "            tokens = tokenized_sentence.copy()\n",
    "            tokens[index] = fill_tokenizer.mask_token\n",
    "            input_ids = fill_tokenizer.convert_tokens_to_ids(tokens)\n",
    "            inputs = torch.tensor([input_ids]).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = fill_model(inputs)\n",
    "                predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            predicted_token = fill_tokenizer.convert_ids_to_tokens(predictions[0, index].item())\n",
    "            predicted_tokens.append(predicted_token)\n",
    "\n",
    "        new_sentence = tokenized_sentence.copy()\n",
    "\n",
    "        for index, predicted_token in zip(masked_indices, predicted_tokens):\n",
    "            new_sentence[index] = predicted_token\n",
    "\n",
    "        new_sentence = fill_tokenizer.convert_tokens_to_string(new_sentence)\n",
    "        predicted_tokens_list.append(new_sentence)\n",
    "\n",
    "    aug_data = pd.DataFrame({'document': predicted_tokens_list, 'label': label_num})\n",
    "\n",
    "    return aug_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d08386",
   "metadata": {},
   "source": [
    "### 3-1-5. Summarize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20a77f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_sentences(data: pd.DataFrame, label: int, num: int, sum_ratio: int=0.8)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        data (DataFrame): 증강시킬 데이터가 있는 데이터 프레임\n",
    "        label (int): 증강시키고 싶은 label\n",
    "        num (int): 증강시킬 데이터의 수\n",
    "        sum_ratio (int): 요약할 문장 비율\n",
    "        \n",
    "    Output:\n",
    "        augmented_df (pd.DataFrame): 증강시킨 데이터가 있는 데이터 프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    # 데이터 필터링\n",
    "    filtered_data = data[data['label'] == label]['document'].tolist()\n",
    "    \n",
    "    selected_data_set = set()  # 중복 선택된 데이터를 추적하기 위한 집합\n",
    "\n",
    "    # 데이터 증강\n",
    "    augmented_data = []\n",
    "    \n",
    "    for _ in range(num):\n",
    "        selected_data = random.choice(filtered_data)  # 원본 데이터 중에서 무작위로 선택\n",
    "\n",
    "        while selected_data in selected_data_set:\n",
    "            selected_data = random.choice(filtered_data)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "        selected_data_set.add(selected_data)  # 선택된 데이터를 집합에 추가\n",
    "        \n",
    "        sentences = tokenize_sentences(selected_data)\n",
    "        \n",
    "        sen_num = int(len(sentences) * sum_ratio)\n",
    "        \n",
    "        # 데이터 증강\n",
    "        generated_data = kss.summarize_sentences(selected_data, max_sentences=sen_num)\n",
    "        \n",
    "        # 요약된 문장들을 하나의 이어진 문장으로 변환  -> kss.summarize_sentences결과가 ['ab','df','fg'] 이런식으로 나오기 때문\n",
    "        if generated_data:\n",
    "            augmented_text = ' '.join(generated_data)\n",
    "        else:\n",
    "            augmented_text = ''\n",
    "        \n",
    "        # 생성된 데이터를 리스트에 추가\n",
    "        augmented_data.append(augmented_text)\n",
    "\n",
    "    # 생성된 데이터를 DataFrame으로 변환\n",
    "    augmented_df = pd.DataFrame({'document': augmented_data, 'label': label})\n",
    "\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01283d1",
   "metadata": {},
   "source": [
    "## 3-2. 클래스 및 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12213f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=True, train=True):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:  # train data이면 sentence와 label 같이 전처리\n",
    "            labels = self.data['label'][index]\n",
    "            sentence = self.data['document'][index]\n",
    "        else:  # test data이면 label만 같이 전처리\n",
    "            sentence = self.data['document'][index]\n",
    "\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=config.MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "#             return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        padded_token_list = encoded_dict['input_ids'][0]\n",
    "        token_type_id = encoded_dict['token_type_ids'][0]\n",
    "        att_mask = encoded_dict['attention_mask'][0]\n",
    "\n",
    "        if self.train:\n",
    "            target = torch.tensor(labels)\n",
    "            sample = (padded_token_list, token_type_id, att_mask, target)\n",
    "        else:\n",
    "            sample = (padded_token_list, token_type_id, att_mask)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a866333",
   "metadata": {},
   "source": [
    "# 4. 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54cefeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb 기록용\n",
    "hyperparameters={\n",
    "    \"model\": \"klue/roberta-base\",\n",
    "    \"data_ag_method\" : \"False\",\n",
    "    \"Stopword\" : \"True\",    \n",
    "    \"MAX_LEN\" : 512,\n",
    "    \"epochs\" : 3,\n",
    "    \"BATCH_SIZE\" : 4,\n",
    "    'NUM_CORES': 0,\n",
    "    'learning_rate': 1e-5,\n",
    "    'eps' : 1e-8,\n",
    "    'loss' : 'BalancedCrossEntropyLoss'\n",
    "      }\n",
    "\n",
    "#wandb 초기화\n",
    "wandb.init(project='fn_guide_project',name='AD_NEWS_klue/roberta-base', config = hyperparameters)\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b2a9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainTestDataset(train_df, train=True)\n",
    "val_dataset = TrainTestDataset(val_df, train=True)\n",
    "test_dataset = TrainTestDataset(test_df, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c795ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=config.BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=config.NUM_CORES)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                batch_size=config.BATCH_SIZE,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=config.NUM_CORES)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                batch_size=config.BATCH_SIZE,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=config.NUM_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69654948",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=config.learning_rate, eps =config.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5c8f43",
   "metadata": {},
   "source": [
    "# 5. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12a888e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (self.alpha * (1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9966aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Balanced_crossentropy_loss(nn.Module):\n",
    "    def __init__(self, beta=0.5):\n",
    "        super(BalancedCrossEntropyLoss, self).__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: 모델의 출력 logits\n",
    "        # targets: 실제 레이블 (0 또는 1)\n",
    "\n",
    "        # 확률값으로 변환하여 클래스 1에 해당하는 값만 선택\n",
    "        prob = torch.softmax(inputs, dim=1)\n",
    "        prob_class_1 = prob[:, 1]\n",
    "\n",
    "        # 클래스별 샘플 수 계산\n",
    "        class_count = torch.bincount(targets, minlength=2)  # 이진 분류에서는 minlength=2로 설정합니다.\n",
    "\n",
    "        # 클래스 빈도에 반비례하는 가중치 계산\n",
    "        weights = 1.0 / (class_count.float() + 1e-8)\n",
    "\n",
    "        # 클래스별 가중치를 적용하여 손실 계산\n",
    "        weight = weights[targets]\n",
    "\n",
    "        # Balanced Cross Entropy Loss 계산\n",
    "        loss = -weight * prob_class_1\n",
    "\n",
    "        # 모든 샘플에 대한 손실 평균 계산\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b82cd2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighted_crossentropy_loss(nn.Module):\n",
    "    def __init__(self, pos_weight=None, weight=None, reduction='mean'):\n",
    "        super(BinaryWeightedCrossEntropyLoss, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets_one_hot = torch.eye(2)[targets].to(inputs.device)  # one-hot 인코딩, inputs와 동일한 디바이스로 이동\n",
    "        weight_tensor = torch.tensor(self.weight).to(inputs.device)  # weight를 Tensor로 변환, inputs와 동일한 디바이스로 이동\n",
    "        pos_weight_tensor = torch.tensor(self.pos_weight).to(inputs.device)  # pos_weight를 Tensor로 변환, inputs와 동일한 디바이스로 이동\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets_one_hot, pos_weight=pos_weight_tensor, weight=weight_tensor, reduction=self.reduction)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147ad808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "output = os.getenv('HOME')+'/aiffel/project/FnGuide/model/' #자신의 경로로 바꾸기\n",
    "model_name1 = model_name.replace('/','_')\n",
    "\n",
    "# loss_fn = Weighted_crossentropy_loss(pos_weight=3, weight=1)\n",
    "# loss_fn = Balanced_crossentropy_loss(beta=0.5)\n",
    "\n",
    "model.train()\n",
    "\n",
    "# wandb.watch를 통해 모델 가중치를 기록\n",
    "wandb.watch(model)\n",
    "\n",
    "# 학습 및 검증 결과를 로깅할 리스트 초기화\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "# 학습 루프\n",
    "for i in range(config.epochs):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Training loop\n",
    "    for input_ids_batch, token_type_id_batch, attention_masks_batch, y_batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        y_batch = y_batch.to(device)\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        token_type_id_batch = token_type_id_batch.to(device)\n",
    "        attention_masks_batch = attention_masks_batch.to(device)\n",
    "        y_pred = model(input_ids_batch, token_type_ids=token_type_id_batch, attention_mask=attention_masks_batch)[0].to(device)\n",
    "\n",
    "        loss = F.cross_entropy(y_pred, y_batch)  # 일반 loss 사용\n",
    "#         loss = loss_fn(y_pred, y_batch)   # 다른 loss 사용\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        correct += (predicted == y_batch).sum()\n",
    "        total += len(y_batch)\n",
    "\n",
    "    losses.append(total_loss)\n",
    "    accuracies.append(correct.float() / total)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        for val_input_ids_batch, val_token_type_id_batch, val_attention_masks_batch, val_y_batch in tqdm(val_dataloader):\n",
    "            val_y_batch = val_y_batch.to(device)\n",
    "            val_input_ids_batch = val_input_ids_batch.to(device)\n",
    "            val_token_type_id_batch = val_token_type_id_batch.to(device)\n",
    "            val_attention_masks_batch = val_attention_masks_batch.to(device)\n",
    "            val_y_pred = model(val_input_ids_batch, token_type_ids=val_token_type_id_batch, attention_mask=val_attention_masks_batch)[0].to(device)\n",
    "\n",
    "            _, val_predicted = torch.max(val_y_pred, 1)\n",
    "            val_correct += (val_predicted == val_y_batch).sum()\n",
    "            val_total += len(val_y_batch)\n",
    "\n",
    "        val_accuracy = val_correct.float() / val_total\n",
    "        print(\"Train Loss:\", total_loss / total, \"Accuracy:\", correct.float() / total)\n",
    "        print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "    \n",
    "\n",
    "    # wandb에 메트릭 로깅\n",
    "    wandb.log({\"Train Loss\": total_loss / total, \"Train Accuracy\": correct.float() / total, \"Validation Accuracy\": val_accuracy})\n",
    "    model.train()\n",
    "\n",
    "    # 모델 저장\n",
    "    torch.save(model.state_dict(), os.path.join(output, f'{model_name1}_{config.data_ag_method}_{config.loss}_stopword_classification_model_{i+1}.pt')) #모델 저장이름도 자기에게 맞게 바꾸기\n",
    "    wandb.save(os.path.join(output, f'{model_name1}_{config.data_ag_method}_{config.loss}_stopword_classification_model_{i+1}.pt'))  # 모델 wandb에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a6adeb",
   "metadata": {},
   "source": [
    "# 6. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dccf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, token_type_id_batch, attention_masks_batch in tqdm(test_dataloader):\n",
    "        y_pred = model(input_ids_batch.to(device),token_type_ids = token_type_id_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        pred.extend(predicted.tolist())\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(pred, test_df['label'])\n",
    "    print(accuracy_score(pred, test_df['label']))\n",
    "    classification_report_str = classification_report(test_df['label'], pred, target_names=['real_news', 'AD'])\n",
    "    print(classification_report(test_df['label'], pred, target_names=['real_news', 'AD']))\n",
    "\n",
    "    cm = confusion_matrix(test_df['label'], pred)\n",
    "    sns.heatmap(cm, annot = True, cmap='coolwarm', xticklabels=['real_news', 'AD'], yticklabels=['real_news', 'AD'], fmt='d')\n",
    "    plt.show()\n",
    "    \n",
    "    # f1-score 추출\n",
    "    report = classification_report(test_df['label'], pred, target_names=['real_news', 'AD'], output_dict=True)\n",
    "\n",
    "    wandb.log({\"test_accuracy\": accuracy})\n",
    "    wandb.log({\"classification_report\": classification_report_str})\n",
    "    wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                y_true=test_df['label'],\n",
    "                                                                preds=pred,\n",
    "                                                                class_names=['real_news', 'AD'])})\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107503d",
   "metadata": {},
   "source": [
    "## 6-1. 틀린 문장 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check_df = test_df.copy() #테스트 데이터프레임 복사\n",
    "pred1 = pd.Series(pred, name=\"pred\") #예측된 레이블 시리즈로 만들고 이름 붙혀주기\n",
    "test_check_df = pd.concat([test_check_df, pred1], axis = 1) #두개를 하나의 데이터프레임으로 합치기\n",
    "\n",
    "diff_indices = test_check_df[test_check_df['label'] != test_check_df['pred']].index #답변이 틀린것들의 index를 뽑을 수 있음\n",
    "diff_indices\n",
    "#test_check_df.to_csv(data_path +'test_check_df.csv') #혹시나 저장하려면 data_path 잘 확인해서 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f200c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_all_check(diff_indices):\n",
    "    for i in diff_indices:\n",
    "        print(\"인덱스:\", i)\n",
    "        print('실제 답변:{}, 예측 답변:{}'.format(test_check_df['label'][i],test_check_df['pred'][i]))\n",
    "        print('document: \\n',test_check_df['document'][i])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc256c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrong_all_check(diff_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7098ab",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
