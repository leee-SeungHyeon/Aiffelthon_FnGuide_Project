{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69b6034",
   "metadata": {},
   "source": [
    "# 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416d78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset,TensorDataset\n",
    "from torch.nn import functional as F\n",
    "import re\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from konlpy.tag import Mecab\n",
    "import wandb\n",
    "import collections\n",
    "import pickle\n",
    "import kss\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from transformers import set_seed\n",
    "\n",
    "from ktextaug import TextAugmentation\n",
    "import ktextaug\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelForMaskedLM\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.family'] = 'NanumGothic'\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "set_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(random_seed)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad37ed2b",
   "metadata": {},
   "source": [
    "# 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21452df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):#파일이름만 넣기\n",
    "    data_path = os.getenv('HOME') + '/aiffel/project/FnGuide/data/ad_news'#경로는 자신의 경로로 바꿔서 하면 됨\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    df = pd.read_csv(file_path, encoding='utf-8', delimiter='\\t')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec294c",
   "metadata": {},
   "source": [
    "## 1-1 사전학습 모델 및 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"klue/roberta-base\"  # 허깅페이스에서 모델만 갈아끼우면 됨\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 예측하고자 하는 클래스의 수를 넣어줘야함\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f5e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kb-albert-char-base-v2, kfdeberta-base\n",
    "model_name='kfdeberta-base'\n",
    "\n",
    "p_model_path = os.getenv('HOME') + \"/aiffel/project/FnGuide/kfdeberta-base\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(p_model_path, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(p_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d1737",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1b755",
   "metadata": {},
   "source": [
    "#### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3769cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data('preprocess_train.csv')\n",
    "val_df = load_data('preprocess_val.csv')\n",
    "test_df = load_data('preprocess_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d83f9",
   "metadata": {},
   "source": [
    "# Augmentation method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d992a072",
   "metadata": {},
   "source": [
    "### 3-1. Back translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba7d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate(data: pd.DataFrame, label: int, num: int, mode: str=\"back_translate\", target_language: str='en', prob: int=0.4)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            data (pandas.DataFrame) : CSV 파일로 구성된 데이터\n",
    "            label (int) : 증강에 사용할 레이블\n",
    "            num (int) : 증강 개수\n",
    "            mode (str, optional) : 데이터 증강 모드. (back_translate/noise_add), 기본값은 \"back_translate\"\n",
    "            target_language (str, optional) : 번역할 언어. 기본값은 'en' (영어)\n",
    "            prob (float, optional) : noise_add 모드에서의 적용 확률\n",
    "        \n",
    "        Returns:\n",
    "            augmented_df (pd.DataFrame) : 증강된 데이터들로 구성된 DataFrame  \n",
    "    \"\"\"\n",
    "    \n",
    "    # TextAugmentation 객체 생성\n",
    "    agent = TextAugmentation(tokenizer=\"mecab\", num_processes=1)\n",
    "\n",
    "    # 선택한 레이블로 데이터 필터링\n",
    "    filtered_data = data[data['label'] == label]['document'].tolist()\n",
    "\n",
    "    # 데이터 증강\n",
    "    augmented_data = []\n",
    "    selected_data_set = set()  # 중복 선택된 데이터를 추적하기 위한 집합\n",
    "\n",
    "    for _ in range(num):\n",
    "        selected_data = random.choice(filtered_data)  # 원본 데이터 중에서 무작위로 선택\n",
    "\n",
    "        while selected_data in selected_data_set:\n",
    "            selected_data = random.choice(filtered_data)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "        selected_data_set.add(selected_data)  # 선택된 데이터를 집합에 추가\n",
    "\n",
    "        # 데이터 증강\n",
    "        if mode == \"noise_add\":\n",
    "            generated_data = agent.generate(selected_data, mode=\"noise_add\", prob=prob, noise_mode=['phonological_change', 'vowel_change', 'jamo_split'])\n",
    "        else:\n",
    "            generated_data = agent.generate(selected_data, mode=\"back_translate\", target_language=target_language)  #일본어로 하려면 target_language='jp'\n",
    "        \n",
    "        augmented_data.append(generated_data)\n",
    "\n",
    "    # 생성된 데이터를 DataFrame으로 변환\n",
    "    augmented_df = pd.DataFrame({'document': augmented_data, 'label': label})\n",
    "    \n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806f622",
   "metadata": {},
   "source": [
    "### 3-2. Document Mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1c9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_blocks = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01ae76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text: str, mode: str ='kss') -> list :\n",
    "    \"\"\"\n",
    "    문장을 분리하는 함수\n",
    "    \n",
    "    Input:\n",
    "        text (str): 증강할 문장\n",
    "        mode (str): 문장을 분리하는 방식\n",
    "\n",
    "    Output:\n",
    "        sentences(list): 분리된 문장 리스트\n",
    "    \"\"\"\n",
    "    if mode == 'to':\n",
    "        sentences = tokenizer.tokenize(text)  # 토크나이저를 사용하여 문장 단위로 분리\n",
    "\n",
    "    else:\n",
    "        sentences = kss.split_sentences(text)  # Split sentences using kss library\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3324f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_shuffle(text: str, min_blocks: int = min_blocks) -> str:\n",
    "    \"\"\"\n",
    "    하나의 문장내에서 문장 순서를 뒤섞는 함수\n",
    "    \n",
    "    Input:\n",
    "        text (str): 증강할 문장\n",
    "        min_blocks (str): 최소 문장 수\n",
    "\n",
    "    Output:\n",
    "        augmented_text (str): 셔플된 하나의 텍스트\n",
    "    \"\"\"\n",
    "    sentences = tokenize_sentences(text, 'kss') \n",
    "    \n",
    "    # 분리된 문장들 중에서 하나 이상의 문장 블록을 선택하기 위해 최소 블록 개수를 설정\n",
    "    min_blocks = min(min_blocks, len(sentences))\n",
    "    \n",
    "    # 무작위로 두 개 이상의 문장 블록을 선택\n",
    "    num_blocks_to_shuffle = random.randint(min_blocks, len(sentences)) #min_blocks부터 len(sentences) 사이의 정수 에서 무작위로 하나의 정수를 반환하는 함수\n",
    "    selected_blocks_indices = random.sample(range(len(sentences)), num_blocks_to_shuffle)\n",
    "    \n",
    "    # 선택된 문장 블록들의 위치를 서로 바꿈\n",
    "    shuffled_sentences = [sentences[i] for i in range(len(sentences)) if i not in selected_blocks_indices]\n",
    "    for i in selected_blocks_indices:\n",
    "        shuffled_sentences.append(sentences[i])\n",
    "    \n",
    "    # 문장 블록들을 다시 하나의 문자열로 합쳐서 증강된 문장을 생성\n",
    "    augmented_text = ' '.join(shuffled_sentences)\n",
    "    \n",
    "    return augmented_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4d848db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_document(data, label, num, min_blocks = min_blocks, mix_ratio_range=(0.2, 0.4), mode='kss', shuffle_mode=2)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            data (pandas.DataFrame) : CSV 파일로 구성된 데이터\n",
    "            label (int) : 증강에 사용할 레이블\n",
    "            num (int) : 증강 개수\n",
    "            min_blocks (str): 최소 문장 수\n",
    "            mix_ratio_range (tuple) : 데이터에서 사용할 문장 선택 비율, 기본값 0.2~0.4에서 랜덤\n",
    "            mode (str, optional) : 데이터 증강 모드. (kss/tokenize), 기본값은 \"back_translate\"\n",
    "            shuffle_mode (int) : 문장 mix할 개수, (1,2), 기본값은 2\n",
    "        \n",
    "        Returns:\n",
    "            augmented_df (pd.DataFrame) : 증강된 데이터들로 구성된 DataFrame  \n",
    "    \"\"\"\n",
    "    augmented_data = []\n",
    "    filtered_data = data[data['label'] == label]['document'].tolist()\n",
    "    \n",
    "    selected_data_set = set()  # 중복 선택된 쌍을 추적하기 위한 집합\n",
    "    \n",
    "    for _ in range(num):\n",
    "        \n",
    "        if shuffle_mode == 2:\n",
    "        \n",
    "            selected_data = random.sample(filtered_data, 2)  # 원본 데이터 중에서 2개를 무작위로 선택\n",
    "\n",
    "            while tuple(selected_data) in selected_data_set:\n",
    "                selected_data = random.sample(filtered_data, 2)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "            selected_data_set.add(tuple(selected_data))\n",
    "\n",
    "            sentences_1 = tokenize_sentences(selected_data[0], mode)  # 첫 번째 선택된 데이터의 문장으로 분리\n",
    "            sentences_2 = tokenize_sentences(selected_data[1], mode)  # 두 번째 선택된 데이터의 문장으로 분리\n",
    "\n",
    "            mix_ratio_1 = random.uniform(*mix_ratio_range)\n",
    "            mix_ratio_2 = random.uniform(*mix_ratio_range)\n",
    "\n",
    "            # sentences_1에서 특정 비율만큼 문장 제거\n",
    "            num_sentences_to_remove = int(len(sentences_1) * mix_ratio_1)\n",
    "            removed_sentences = random.sample(sentences_1, num_sentences_to_remove)\n",
    "            augmented_sentences_1 = [sentence for sentence in sentences_1 if sentence not in removed_sentences]\n",
    "\n",
    "\n",
    "            # sentences_2에서 특정 비율만큼 문장 추출\n",
    "            num_sentences_to_extract_2 = int(len(sentences_2) * mix_ratio_2)\n",
    "            extracted_sentences_2 = random.sample(sentences_2, num_sentences_to_extract_2)\n",
    "\n",
    "            # 추출된 문장들을 랜덤하게 위치에 삽입하여 augmented_sentences_1에 삽입\n",
    "            for sentence in extracted_sentences_2:\n",
    "                random_index = random.randint(0, len(augmented_sentences_1))\n",
    "                augmented_sentences_1.insert(random_index, sentence)\n",
    "\n",
    "            if mode == 'to':\n",
    "                augmented_text = tokenizer.convert_tokens_to_string(augmented_sentences_1)  # 토큰을 다시 문자열로 변환하여 문서로 만듦\n",
    "            else:\n",
    "                augmented_text = ' '.join(augmented_sentences_1)  # 문장 내의 단어들을 공백으로 연결하여 문장으로 만듦\n",
    "\n",
    "            augmented_data.append(augmented_text)\n",
    "            \n",
    "        elif shuffle_mode == 1:\n",
    "        \n",
    "            selected_data = random.choice(filtered_data)# 원본 데이터 중에서 1개를 무작위로 선택\n",
    "\n",
    "            while tuple(selected_data) in selected_data_set:\n",
    "                selected_data = random.choice(filtered_data)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "            selected_data_set.add(tuple(selected_data))\n",
    "\n",
    "            augmented_text = sentence_shuffle(selected_data, min_blocks)\n",
    "            augmented_data.append(augmented_text)\n",
    "        \n",
    "    # 생성된 증강 데이터에 레이블 1 추가\n",
    "    augmented_df = pd.DataFrame({'document': augmented_data, 'label': label})\n",
    "        \n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7a0f0",
   "metadata": {},
   "source": [
    "### 3-3. KoEDA\n",
    "\n",
    "p = (alpha_sr, alpha_ri, alpha_rs, prob_rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75652d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from koeda import EDA\n",
    "\n",
    "def ag_koeda(data: pd.DataFrame, label: int, num: int, p: tuple=(0.3, 0.3, 0.3, 0.3))-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        data (DataFrame): 증강시킬 데이터가 있는 데이터 프레임\n",
    "        label (int): 증강시키고 싶은 label\n",
    "        num (int): 증강시킬 데이터의 수\n",
    "        p (tuple): alpha_sr, alpha_ri, alpha_rs, prob_rd 기법의 수치\n",
    "        \n",
    "    Output:\n",
    "        augmented_df (DataFrame): 증강시킨 데이터가 있는 데이터 프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    eda = EDA(morpheme_analyzer=\"Mecab\", alpha_sr=p[0], alpha_ri=p[1], alpha_rs=p[2], prob_rd=p[3])\n",
    "\n",
    "    # 데이터 필터링\n",
    "    filtered_data = data[data['label'] == label]['document'].tolist()\n",
    "    \n",
    "    selected_data_set = set()  # 중복 선택된 데이터를 추적하기 위한 집합\n",
    "\n",
    "    # 데이터 증강\n",
    "    augmented_data = []\n",
    "    for _ in range(num):\n",
    "        selected_data = random.choice(filtered_data)  # 원본 데이터 중에서 무작위로 선택\n",
    "\n",
    "        while selected_data in selected_data_set:\n",
    "            selected_data = random.choice(filtered_data)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "        selected_data_set.add(selected_data)  # 선택된 데이터를 집합에 추가\n",
    "\n",
    "        # 데이터 증강\n",
    "        generated_data = eda(selected_data, p=p, repetition=1)  # repetition: 한 문장을 몇 개로 증강시킬 것인지\n",
    "        augmented_data.append(generated_data)\n",
    "\n",
    "    # 생성된 데이터를 DataFrame으로 변환\n",
    "    augmented_df = pd.DataFrame({'document': augmented_data, 'label': label})\n",
    "\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6c1161",
   "metadata": {},
   "source": [
    "### 3-4. Fill Masked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65b979fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_masked_augmentation(df: pd.DataFrame, label_num: int, random_number: int, num_masks_ratio: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        df (DataFrame): 증강시킬 데이터가 있는 데이터 프레임\n",
    "        label_num (int): 증강시키고 싶은 label number\n",
    "        random_number (int): 증강시킬 데이터의 수\n",
    "        num_masks_ratio (float): 본문에서 마스킹 시킬 비율\n",
    "        max_length (int): sentence 토큰의 최대 길이\n",
    "        \n",
    "    Output:\n",
    "        aug_data (DataFrame): 증강시킨 데이터가 있는 데이터 프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    label_data = df[df['label'] == label_num]\n",
    "    predicted_tokens_list = []\n",
    "    fill_model.to(device)\n",
    "    random.seed(random_seed) #이렇게 하니까 증강문이 고정됨(마스킹도 고정)\n",
    "    \n",
    "    for _ in tqdm(range(random_number)):\n",
    "        \n",
    "        random_sentences = random.choice(label_data['document'].tolist()) \n",
    "\n",
    "        num_masks = int(len(random_sentences.split()) * num_masks_ratio)\n",
    "        num_masks = min(num_masks, 20)\n",
    "        words = random_sentences.split()\n",
    "        masked_indices = random.sample(range(len(words)), num_masks)\n",
    "        masked_text = ' '.join('[MASK]' if i in masked_indices else word for i, word in enumerate(words))\n",
    "        tokenized_sentence = fill_tokenizer.tokenize(masked_text)\n",
    "        masked_indices = [i for i, token in enumerate(tokenized_sentence) if token == fill_tokenizer.mask_token]\n",
    "        \n",
    "        predicted_tokens = []\n",
    "        for index in masked_indices:\n",
    "            tokens = tokenized_sentence.copy()\n",
    "            tokens[index] = fill_tokenizer.mask_token\n",
    "            input_ids = fill_tokenizer.convert_tokens_to_ids(tokens)\n",
    "            inputs = torch.tensor([input_ids]).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = fill_model(inputs)\n",
    "                predictions = outputs.logits[0, index].topk(k=5)\n",
    "            \n",
    "            max_count = 5\n",
    "            count = 0\n",
    "            random_prediction_index = torch.multinomial(predictions.values, 1)  # 상위 5개 중에서 랜덤하게 하나 선택\n",
    "            random_prediction_token_id = predictions.indices[random_prediction_index].item()  # 선택한 토큰 ID 추출\n",
    "            random_prediction_token = fill_tokenizer.convert_ids_to_tokens([random_prediction_token_id])[0]  # 토큰으로 변환\n",
    "            \n",
    "            while random_prediction_token in [\"(\",\")\",\" \",\".\", \",\", \"'\", '\"',\"‘\",\"’\",'“','”',\"[SEP]\", \"[PAD]\", \"[UNK]\"]:\n",
    "                count += 1\n",
    "                if count >= max_count:\n",
    "                    random_prediction_token = \" \"\n",
    "                    break\n",
    "                random_prediction_index = torch.multinomial(predictions.values, 1)  # 상위 5개 중에서 랜덤하게 하나 선택\n",
    "                random_prediction_token_id = predictions.indices[random_prediction_index].item()  # 선택한 토큰 ID 추출\n",
    "                random_prediction_token = fill_tokenizer.convert_ids_to_tokens([random_prediction_token_id])[0]  # 토큰으로 변환\n",
    "                \n",
    "            predicted_tokens.append(random_prediction_token)\n",
    "\n",
    "        new_sentence = tokenized_sentence.copy()\n",
    "\n",
    "        for index, predicted_token in zip(masked_indices, predicted_tokens):\n",
    "            new_sentence[index] = predicted_token\n",
    "\n",
    "        new_sentence = fill_tokenizer.convert_tokens_to_string(new_sentence).strip()\n",
    "        predicted_tokens_list.append(new_sentence)\n",
    "\n",
    "    aug_data = pd.DataFrame({'document': predicted_tokens_list, 'label': label_num})\n",
    "\n",
    "    return aug_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4eb84",
   "metadata": {},
   "source": [
    "### 3-5. Summarize sentences\n",
    "#### 3-5-1. 추출적 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf2783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_function(text: str, summarize_ratio: float) -> str:\n",
    "    '''\n",
    "    텍스트를 요약시키는 함수\n",
    "    \n",
    "    Input:\n",
    "        text (str): 요약시킬 텍스트\n",
    "        summarize_ratio (float): 요약 비율\n",
    "       \n",
    "    Output:\n",
    "        summraize_text (str): 요약된 텍스트    \n",
    "    '''\n",
    "    summraize_text = summarize(text, ratio = summarize_ratio)\n",
    "    summraize_text = summraize_text.replace('\\n', ' ')\n",
    "    \n",
    "    return summraize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3fae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_summarize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    임베딩된 텍스트의 길이가 512가 넘으면 텍스트를 요약하여 요약된 데이터를 이용한 데이터프레임을 만드는 함수\n",
    "    \n",
    "    Input:\n",
    "        df (DataFrame): 요약시킬 데이터가 있는 데이터프레임\n",
    "       \n",
    "    Output:\n",
    "        augmented_df (DataFrame): 요약된 텍스트가 있는 데이터프레임  \n",
    "    '''\n",
    "    \n",
    "    summarize_data = []\n",
    "    labels = []\n",
    "    index = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        if 512 < len(tokenizer.encode(df['document'][i])):\n",
    "            aa = summarize_function(df['document'][i], 0.5)\n",
    "            summarize_data.append(aa)\n",
    "            labels.append(df['label'][i])\n",
    "            index.append(i)\n",
    "    augmented_df = pd.DataFrame({'document': summarize_data, 'label': labels, 'index': index})\n",
    "    \n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8aca9",
   "metadata": {},
   "source": [
    "#### 3-5-2. 추상적 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e8de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "class KoBARTConditionalGeneration(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KoBARTConditionalGeneration, self).__init__()\n",
    "        self.model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v2')\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        attention_mask = inputs['input_ids'].ne(self.pad_token_id).float()\n",
    "        decoder_attention_mask = inputs['decoder_input_ids'].ne(self.pad_token_id).float()\n",
    "\n",
    "        return self.model(input_ids=inputs['input_ids'],\n",
    "                          attention_mask=attention_mask,\n",
    "                          decoder_input_ids=inputs['decoder_input_ids'],\n",
    "                          decoder_attention_mask=decoder_attention_mask,\n",
    "                          labels=inputs['labels'], return_dict=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = KoBARTConditionalGeneration().to(device)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99594f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성\n",
    "model_wrapper = KoBARTConditionalGeneration().to(device)\n",
    "\n",
    "# 가중치 로드\n",
    "model_path_gen = os.getenv('HOME') + \"/aiffel/project/FnGuide/model/best_model.pt\"\n",
    "model_wrapper.load_state_dict(torch.load(model_path_gen))\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model_wrapper.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d05cbfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sentences(sentences, ratio):\n",
    "    num_groups = len(sentences) // ratio\n",
    "    grouped_sentences = [''.join(sentences[i : i + ratio]) for i in range(0, len(sentences), ratio)]\n",
    "    return grouped_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f4e842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰기준 512넘어가는 데이터 레이블 리스트\n",
    "\n",
    "file_name = '512_over_text_index.txt'\n",
    "\n",
    "with open(file_name, 'r') as file:\n",
    "    list_str = file.read()\n",
    "\n",
    "gen_index_list = [int(item) for item in list_str.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d0f51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(df: pd.DataFrame, gen_index_list : list) -> pd.DataFrame:\n",
    "    summarize_data = []\n",
    "    labels = []\n",
    "    index = []\n",
    "\n",
    "    for j in tqdm(range(len(gen_index_list))):\n",
    "        i = gen_index_list[j]\n",
    "        sentence = df['document'][i]\n",
    "        text = tokenize_sentences(sentence)\n",
    "        ratio = len(text) // 5\n",
    "        t_text = group_sentences(text, ratio)\n",
    "\n",
    "        p_text = []\n",
    "        for text in t_text:\n",
    "            input_ids = tokenizer.encode(text)\n",
    "            input_ids = torch.tensor(input_ids)\n",
    "            input_ids = input_ids.unsqueeze(0).to(device)\n",
    "            output = model_wrapper.model.generate(input_ids, eos_token_id=1, max_length=256, num_beams=10)\n",
    "            output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            p_text.append(output)\n",
    "\n",
    "        gen_text = ' '.join(p_text)\n",
    "        summarize_data.append(gen_text)\n",
    "        labels.append(df['label'][i])\n",
    "        index.append(i)\n",
    "            \n",
    "    augmented_df = pd.DataFrame({'document': summarize_data, 'label': labels, 'index': index})\n",
    "    \n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd20bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_aug = gen_text(train_df, gen_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강된것 저장\n",
    "gen_aug.to_csv(os.getenv('HOME') + '/aiffel/project/FnGuide/data/ad_news/gen_train.csv', index=False, encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train = pd.read_csv(os.getenv('HOME') + '/aiffel/project/FnGuide/data/ad_news/summarized_train.csv', sep='\\t')\n",
    "gen_train = gen_train[['document', 'label']] #이거 하고 해야 오류 안남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 증강된 것 포함해서 실험 진행\n",
    "gen_data = pd.concat([train_df, gen_train]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c9729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 증강된 것 대체해서 실험 진행\n",
    "index_list = gen_train.index.tolist()\n",
    "\n",
    "for i in gen_train['index']:\n",
    "    for j in index_list:\n",
    "        if i == j:\n",
    "            train_df['document'][j] = gen_train['document'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c64f15",
   "metadata": {},
   "source": [
    "# 4. Loss function\n",
    "### 4-1. Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae162ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (self.alpha * (1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c6e5c2",
   "metadata": {},
   "source": [
    "### 4-2 Hinge Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(HingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # 이진 분류를 위해 출력 텐서를 (배치 크기, 2) 모양으로 변경\n",
    "        outputs = outputs.view(-1, 2)\n",
    "\n",
    "        # 모델의 출력 디바이스에 맞추기\n",
    "        targets = targets.to(outputs.device)\n",
    "\n",
    "        # 이중 손실 계산\n",
    "        loss = torch.max(torch.tensor(0.0).to(outputs.device), 1 - outputs * (targets * 2 - 1))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d007d",
   "metadata": {},
   "source": [
    "### 4-3 Balanced Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, beta=0.5):\n",
    "        super(BalancedCrossEntropyLoss, self).__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: 모델의 출력 logits\n",
    "        # targets: 실제 레이블 (0 또는 1)\n",
    "\n",
    "        # 확률값으로 변환하여 클래스 1에 해당하는 값만 선택\n",
    "        prob = torch.softmax(inputs, dim=1)\n",
    "        prob_class_1 = prob[:, 1]\n",
    "\n",
    "        # 클래스별 샘플 수 계산\n",
    "        class_count = torch.bincount(targets, minlength=2)  # 이진 분류에서는 minlength=2로 설정합니다.\n",
    "\n",
    "        # 클래스 빈도에 반비례하는 가중치 계산\n",
    "        weights = 1.0 / (class_count.float() + 1e-8)\n",
    "\n",
    "        # 클래스별 가중치를 적용하여 손실 계산\n",
    "        weight = weights[targets]\n",
    "\n",
    "        # Balanced Cross Entropy Loss 계산\n",
    "        loss = -weight * prob_class_1\n",
    "\n",
    "        # 모든 샘플에 대한 손실 평균 계산\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0f8d1",
   "metadata": {},
   "source": [
    "### 4-4. Binary Weighted Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryWeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, pos_weight=None, weight=None, reduction='mean'):\n",
    "        super(BinaryWeightedCrossEntropyLoss, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets_one_hot = torch.eye(2)[targets].to(inputs.device)  # one-hot 인코딩, inputs와 동일한 디바이스로 이동\n",
    "        weight_tensor = torch.tensor(self.weight).to(inputs.device)  # weight를 Tensor로 변환, inputs와 동일한 디바이스로 이동\n",
    "        pos_weight_tensor = torch.tensor(self.pos_weight).to(inputs.device)  # pos_weight를 Tensor로 변환, inputs와 동일한 디바이스로 이동\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets_one_hot, pos_weight=pos_weight_tensor, weight=weight_tensor, reduction=self.reduction)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7beafc",
   "metadata": {},
   "source": [
    "# 5. 모델 훈련 및 성능 검증\n",
    "### 5-1. 클래스 및 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d55cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=True, train=True):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:  # train data이면 sentence와 label 같이 전처리\n",
    "            labels = self.data['label'][index]\n",
    "            sentence = self.data['document'][index]\n",
    "        else:  # test data이면 label만 같이 전처리\n",
    "            sentence = self.data['document'][index]\n",
    "\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=config.MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "#             return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        padded_token_list = encoded_dict['input_ids'][0]\n",
    "        token_type_id = encoded_dict['token_type_ids'][0]\n",
    "        att_mask = encoded_dict['attention_mask'][0]\n",
    "\n",
    "        if self.train:\n",
    "            target = torch.tensor(labels)\n",
    "            sample = (padded_token_list, token_type_id, att_mask, target)\n",
    "        else:\n",
    "            sample = (padded_token_list, token_type_id, att_mask)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f03887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=3, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            \n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "324551cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_check(diff_indices, new_df):\n",
    "    for i in diff_indices:\n",
    "        print(\"인덱스:\", i)\n",
    "        print(\"Real news(%) : \", new_df['probability_negative'][i])\n",
    "        print(\"AD news(%) : \", new_df['probability_positive'][i])\n",
    "        print('실제 답변:{}, 예측 답변:{}'.format(new_df['label'][i],new_df['predicted_label'][i]))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c2911",
   "metadata": {},
   "source": [
    "##  5-1. 증강법적용 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fbc296",
   "metadata": {},
   "source": [
    "#### 1. Back trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fb5928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = back_translate(data=train_df, label=1, num=1000, mode=\"back_translate\", target_language='en')\n",
    "train_back = pd.concat([train_df, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d62258",
   "metadata": {},
   "source": [
    "#### 2. mix document\n",
    "##### 2-1 문장 2개 mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7392331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Oh! You have mecab in your environment. Kss will take this as a backend! :D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_df_1 = mix_document(data=train_df, label=1, num=1000, min_blocks = min_blocks, mix_ratio_range=(0.2, 0.4), mode='kss', shuffle_mode=2)\n",
    "train_mix_1 = pd.concat([train_df, augmented_df_1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c9a6c",
   "metadata": {},
   "source": [
    "##### 2-2 문장 1개 mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e60ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df_2 = mix_document(data=train_df, label=1, num=1000, min_blocks = min_blocks, mix_ratio_range=(0.2, 0.4), mode='kss', shuffle_mode=1)\n",
    "train_mix_2 = pd.concat([train_df, augmented_df_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84dd1c",
   "metadata": {},
   "source": [
    "##### 2-3 문장 2개 + 1개 혼합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecb1e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df_3_1 = mix_document(data=train_df, label=1, num=500, min_blocks = min_blocks, mix_ratio_range=(0.2, 0.4), mode='kss', shuffle_mode=2)\n",
    "augmented_df_3_2 = mix_document(data=train_df, label=1, num=500, min_blocks = min_blocks, mix_ratio_range=(0.2, 0.4), mode='kss', shuffle_mode=1)\n",
    "\n",
    "train_mix_3 = pd.concat([train_df, augmented_df_3_1, augmented_df_3_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53203358",
   "metadata": {},
   "source": [
    "#### 3. KoEDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "139dc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = ag_koeda(data=train_df, label=1, num=1000, p=(0.2, 0.1, 0.3, 0.1))\n",
    "train_koeda = pd.concat([train_df, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e52d8",
   "metadata": {},
   "source": [
    "#### 4. Fill masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a12240",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_model_name = \"monologg/kobigbird-bert-base\"  # 허깅페이스에서 모델만 갈아끼우면 됨\n",
    "fill_model = AutoModelForMaskedLM.from_pretrained(fill_model_name)\n",
    "fill_tokenizer = AutoTokenizer.from_pretrained(fill_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ac30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = fill_masked_augmentation(train_df, label_num = 1, random_number = 1000, num_masks_ratio = 0.15)\n",
    "train_fill = pd.concat([train_df, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e19b442",
   "metadata": {},
   "source": [
    "## 5-2. 실험(원하는 증강법 데이터를 적용시켜서 학습)\n",
    "#### ex) Back translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb 기록용\n",
    "hyperparameters={\n",
    "    \"model\": \"klue/roberta-base\",\n",
    "    \"data_ag_method\" : \"back_trans\",\n",
    "    \"Stopword\" : \"True\",    \n",
    "    \"MAX_LEN\" : 512,\n",
    "    \"epochs\" : 3,\n",
    "    \"BATCH_SIZE\" : 8,\n",
    "    'NUM_CORES': 0,\n",
    "    'learning_rate': 2e-5,\n",
    "    'eps' : 1e-8,\n",
    "    'loss' : 'cross_entropy'\n",
    "      }\n",
    "\n",
    "#wandb 초기화\n",
    "wandb.init(project='fn_guide_project',name='AD_NEWS_klue/roberta-base', config = hyperparameters)\n",
    "config = wandb.config\n",
    "\n",
    "train_dataset = TrainTestDataset(train_back, train=True)\n",
    "val_dataset = TrainTestDataset(val_df, train=True)\n",
    "test_dataset = TrainTestDataset(test_df, train=False)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=config.BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=config.NUM_CORES)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                batch_size=config.BATCH_SIZE,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=config.NUM_CORES)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                batch_size=config.BATCH_SIZE,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=config.NUM_CORES)\n",
    "\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=config.learning_rate, eps =config.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab0e8044",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [11:18<00:00,  1.18it/s]\n",
      "100%|██████████| 85/85 [00:26<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:\n",
      "Train Loss: 0.01467119010777354 Accuracy: tensor(0.9588, device='cuda:0') F1 Score (Train): 0.9594\n",
      "Train Precision: 0.9541 Train Recall: 0.9648\n",
      "Validation Accuracy: tensor(0.9573, device='cuda:0') F1 Score (Validation): 0.9490\n",
      "Validation Precision: 0.9507 Validation Recall: 0.9474\n",
      "Validation loss decreased (inf --> 1.030100).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "100%|██████████| 802/802 [11:17<00:00,  1.18it/s]\n",
      "100%|██████████| 85/85 [00:26<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:\n",
      "Train Loss: 0.0071782427284279105 Accuracy: tensor(0.9824, device='cuda:0') F1 Score (Train): 0.9825\n",
      "Train Precision: 0.9818 Train Recall: 0.9833\n",
      "Validation Accuracy: tensor(0.9602, device='cuda:0') F1 Score (Validation): 0.9537\n",
      "Validation Precision: 0.9329 Validation Recall: 0.9754\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [11:17<00:00,  1.18it/s]\n",
      "100%|██████████| 85/85 [00:27<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:\n",
      "Train Loss: 0.004262124366435507 Accuracy: tensor(0.9874, device='cuda:0') F1 Score (Train): 0.9875\n",
      "Train Precision: 0.9879 Train Recall: 0.9870\n",
      "Validation Accuracy: tensor(0.9588, device='cuda:0') F1 Score (Validation): 0.9512\n",
      "Validation Precision: 0.9446 Validation Recall: 0.9579\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "output = os.getenv('HOME')+'/aiffel/project/FnGuide/model/' #자신의 경로로 바꾸기\n",
    "model_name1 = model_name.replace('/','_')\n",
    "\n",
    "folder_name = f\"{model_name1}_{config.data_ag_method}_{config.loss}\"\n",
    "model_save_folder = os.path.join(output, folder_name)\n",
    "os.makedirs(model_save_folder, exist_ok=True)  # 폴더가 없으면 생성\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, verbose=True, path=os.path.join(model_save_folder, f'{model_name1}_{config.data_ag_method}_{config.loss}_stopword_classification_model.pt'))\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "wandb.watch(model,log=\"all\",log_freq=20)  #model: 모니터링할 모델 객체/log:기록할 항목을 지정하는 옵션/log_freq:기록 빈도\n",
    "\n",
    "# loss_fn = Balanced_crossentropy_loss(beta=0.3)\n",
    "# loss_fn = Weighted_crossentropy_loss(pos_weight=3, weight=1)\n",
    "\n",
    "for i in range(config.epochs):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true_train = []  # 학습 데이터의 실제 라벨을 저장할 리스트\n",
    "    y_pred_train = []  # 학습 데이터의 예측 라벨을 저장할 리스트\n",
    "\n",
    "    # Training loop\n",
    "    for input_ids_batch, token_type_id_batch, attention_masks_batch, y_batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        y_batch = y_batch.to(device)\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        token_type_id_batch = token_type_id_batch.to(device)\n",
    "        attention_masks_batch = attention_masks_batch.to(device)\n",
    "        y_pred = model(input_ids_batch, token_type_ids=token_type_id_batch, attention_mask=attention_masks_batch)[0].to(device)\n",
    "        \n",
    "#         loss = loss_fn(y_pred, y_batch) #Balanced_crossentropy_loss 사용시\n",
    "#         loss = loss_function(y_pred, y_batch.view(-1, 1)) #hingeloss 사용시\n",
    "#         loss = FocalLoss()(y_pred, y_batch) #focalloss 사용시\n",
    "        loss = F.cross_entropy(y_pred, y_batch)  # cross_entropy 사용시\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        correct += (predicted == y_batch).sum()\n",
    "        total += len(y_batch)\n",
    "        \n",
    "        # 학습 데이터의 실제 라벨과 예측 라벨을 저장하여 나중에 F1 점수 계산에 사용\n",
    "        y_true_train.extend(y_batch.tolist())\n",
    "        y_pred_train.extend(predicted.tolist())\n",
    "\n",
    "    losses.append(total_loss)\n",
    "    accuracies.append(correct.float() / total)\n",
    "    \n",
    "    f1_train = f1_score(y_true_train, y_pred_train, average='binary')\n",
    "    precision_train = precision_score(y_true_train, y_pred_train, average='binary')\n",
    "    recall_train = recall_score(y_true_train, y_pred_train, average='binary')\n",
    "    f1_scores.append(f1_train)\n",
    "\n",
    "    y_true_val = []  # 검증 데이터의 실제 라벨을 저장할 리스트 초기화\n",
    "    y_pred_val = []  # 검증 데이터의 예측 라벨을 저장할 리스트 초기화\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0 #추가\n",
    "\n",
    "        for val_input_ids_batch, val_token_type_id_batch, val_attention_masks_batch, val_y_batch in tqdm(val_dataloader):\n",
    "            val_y_batch = val_y_batch.to(device)\n",
    "            val_input_ids_batch = val_input_ids_batch.to(device)\n",
    "            val_token_type_id_batch = val_token_type_id_batch.to(device)\n",
    "            val_attention_masks_batch = val_attention_masks_batch.to(device)\n",
    "            val_y_pred = model(val_input_ids_batch, token_type_ids=val_token_type_id_batch, attention_mask=val_attention_masks_batch)[0].to(device)\n",
    "\n",
    "            _, val_predicted = torch.max(val_y_pred, 1)\n",
    "            val_correct += (val_predicted == val_y_batch).sum()\n",
    "            val_total += len(val_y_batch)\n",
    "            \n",
    "            # 검증 데이터의 실제 라벨과 예측 라벨을 저장\n",
    "            y_true_val.extend(val_y_batch.tolist())\n",
    "            y_pred_val.extend(val_predicted.tolist())\n",
    "        \n",
    "        # 검증 데이터의 F1 점수 계산\n",
    "        f1_val = f1_score(y_true_val, y_pred_val, average='binary')\n",
    "        precision_val = precision_score(y_true_val, y_pred_val, average='binary')\n",
    "        recall_val = recall_score(y_true_val, y_pred_val, average='binary')\n",
    "\n",
    "        val_accuracy = val_correct.float() / val_total\n",
    "        val_loss = F.cross_entropy(val_y_pred, val_y_batch)  # validation loss 값 할당\n",
    "        \n",
    "        print(f\"Epoch {i+1}/{config.epochs}:\")\n",
    "        print(\"Train Loss:\", total_loss / total, \"Accuracy:\", correct.float() / total, \"F1 Score (Train):\", f\"{f1_train:.4f}\")\n",
    "        print(\"Train Precision:\", f\"{precision_train:.4f}\", \"Train Recall:\", f\"{recall_train:.4f}\")\n",
    "        print(\"Validation Accuracy:\", val_accuracy, \"F1 Score (Validation):\", f\"{f1_val:.4f}\")\n",
    "        print(\"Validation Precision:\", f\"{precision_val:.4f}\", \"Validation Recall:\", f\"{recall_val:.4f}\")\n",
    "    \n",
    "    wandb.log({\n",
    "        \"Train Loss\": total_loss / total,\n",
    "        \"Train Accuracy\": correct.float() / total,\n",
    "        \"Validation Accuracy\": val_accuracy,\n",
    "        \"Train F1 Score\": f1_train,\n",
    "        \"Validation F1 Score\": f1_val\n",
    "    })\n",
    "    model.train()\n",
    "         \n",
    "    # Early Stopping 적용\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping!! Goog luck!\")\n",
    "        break\n",
    "        \n",
    "    # 모델 저장  \n",
    "    model_save_path = os.path.join(model_save_folder, f'{model_name1}_{config.data_ag_method}_{config.loss}_stopword_classification_model_{i+1}.pt')\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    wandb.save(os.path.join(model_save_folder, f'{model_name1}_{config.data_ag_method}_{config.loss}_stopword_classification_model_{i+1}.pt'))  # 모델 wandb에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8dc201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:26<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9631811487481591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   real_news       0.95      0.99      0.97       389\n",
      "          AD       0.98      0.93      0.96       290\n",
      "\n",
      "    accuracy                           0.96       679\n",
      "   macro avg       0.97      0.96      0.96       679\n",
      "weighted avg       0.96      0.96      0.96       679\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>▁▇█</td></tr><tr><td>Train F1 Score</td><td>▁▇█</td></tr><tr><td>Train Loss</td><td>█▃▁</td></tr><tr><td>Validation Accuracy</td><td>▁█▄</td></tr><tr><td>Validation F1 Score</td><td>▁█▄</td></tr><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>0.98737</td></tr><tr><td>Train F1 Score</td><td>0.98747</td></tr><tr><td>Train Loss</td><td>0.00426</td></tr><tr><td>Validation Accuracy</td><td>0.95876</td></tr><tr><td>Validation F1 Score</td><td>0.95122</td></tr><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>test_accuracy</td><td>0.96318</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AD_NEWS_klue/roberta-base</strong> at: <a href='https://wandb.ai/fn_guide_project/fn_guide_project/runs/sn6f6n0u' target=\"_blank\">https://wandb.ai/fn_guide_project/fn_guide_project/runs/sn6f6n0u</a><br/> View job at <a href='https://wandb.ai/fn_guide_project/fn_guide_project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg3MTY1MTk1/version_details/v5' target=\"_blank\">https://wandb.ai/fn_guide_project/fn_guide_project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg3MTY1MTk1/version_details/v5</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230806_085742-sn6f6n0u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "pred = []\n",
    "probabilities = [] # 예측 확률값을 저장할 리스트\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, token_type_id_batch, attention_masks_batch in tqdm(test_dataloader):\n",
    "        y_pred = model(input_ids_batch.to(device),token_type_ids = token_type_id_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        pred.extend(predicted.tolist())\n",
    "        probabilities.extend(F.softmax(y_pred, dim=1).tolist())\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(pred, test_df['label'])\n",
    "    print(accuracy_score(pred, test_df['label']))\n",
    "    classification_report_str = classification_report(test_df['label'], pred, target_names=['real_news', 'AD'])\n",
    "    print(classification_report(test_df['label'], pred, target_names=['real_news', 'AD']))\n",
    "\n",
    "    cm = confusion_matrix(test_df['label'], pred)\n",
    "    sns.heatmap(cm, annot = True, cmap='coolwarm', xticklabels=['real_news', 'AD'], yticklabels=['real_news', 'AD'], fmt='d')\n",
    "    plt.show()\n",
    "\n",
    "    # f1-score 추출\n",
    "    report = classification_report(test_df['label'], pred, target_names=['real_news', 'AD'], output_dict=True)\n",
    "\n",
    "    wandb.log({\"test_accuracy\": accuracy})\n",
    "    wandb.log({\"classification_report\": classification_report_str})\n",
    "    wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                y_true=test_df['label'],\n",
    "                                                                preds=pred,\n",
    "                                                                class_names=['negative', 'positive'])})\n",
    "\n",
    "wandb.finish() #훈련과 평가가 다 끝났을때 해주는 것이 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4581c",
   "metadata": {},
   "source": [
    "- 예측 실패 인덱스, softmax값, 맞춘 인덱스 중 softmax 값이 낮은 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4414833",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check_df = test_df.copy() #테스트 데이터프레임 복사\n",
    "pred1 = pd.Series(pred, name=\"pred\") #예측된 레이블 시리즈로 만들고 이름 붙혀주기\n",
    "test_check_df = pd.concat([test_check_df, pred1], axis = 1) #두개를 하나의 데이터프레임으로 합치기\n",
    "\n",
    "diff_indices = test_check_df[test_check_df['label'] != test_check_df['pred']].index #답변이 틀린것들의 index를 뽑을 수 있음\n",
    "\n",
    "print(\"예측 실패 인덱스 : \", diff_indices)\n",
    "#######################################################################################################################################\n",
    "new_df = test_df.copy()\n",
    "new_df['predicted_label'] = pred\n",
    "new_df['probability_negative'] = [prob[0] for prob in probabilities]\n",
    "new_df['probability_positive'] = [prob[1] for prob in probabilities]\n",
    "\n",
    "softmax_check(diff_indices, new_df)\n",
    "\n",
    "negative_lowest_5 = new_df[(new_df['label'] == 0) & (new_df['label'] == new_df['predicted_label'])].nsmallest(5, 'probability_negative').index\n",
    "positive_lowest_5 = new_df[(new_df['label'] == 1) & (new_df['label'] == new_df['predicted_label'])].nsmallest(5, 'probability_positive').index\n",
    "\n",
    "print(\"Label 0일 때 Probability Negative 값이 가장 낮은 5개 인덱스:\", negative_lowest_5)\n",
    "print(\"Label 1일 때 Probability Positive 값이 가장 낮은 5개 인덱스:\", positive_lowest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296d06e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T03:13:32.132321Z",
     "start_time": "2023-08-16T03:13:32.087319Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "예측 실패 인덱스 :  Int64Index([ 40,  71,  73, 158, 161, 188, 192, 244, 246, 275, 291, 303, 324,\n",
    "            343, 372, 403, 456, 473, 495, 500, 554, 556, 569, 618, 643],\n",
    "           dtype='int64')\n",
    "\n",
    "인덱스: 40\n",
    "Real news(%) :  0.9963169097900391\n",
    "AD news(%) :  0.0036830061580985785\n",
    "실제 답변:1, 예측 답변:0\n",
    "\n",
    "\n",
    "인덱스: 71\n",
    "Real news(%) :  0.9993676543235779\n",
    "AD news(%) :  0.0006323348497971892\n",
    "실제 답변:1, 예측 답변:0\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "Label 0일 때 Probability Negative 값이 가장 낮은 5개 인덱스: Int64Index([505, 439, 395, 536, 385], dtype='int64')\n",
    "Label 1일 때 Probability Positive 값이 가장 낮은 5개 인덱스: Int64Index([104, 156, 492, 390, 585], dtype='int64')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b548b9",
   "metadata": {},
   "source": [
    "## 5-2. 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b3c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f112e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"klue/roberta-base\"  # 허깅페이스에서 모델만 갈아끼우면 됨\n",
    "model_name_1 = \"klue_roberta-base_Summarize sentences_cross_entropy_stopword_classification_model_5406.pt\" \n",
    "model_path = os.getenv('HOME') + \"/aiffel/project/FnGuide/model/klue_roberta-base_Summarize sentences_cross_entropy_stopword_classification_model_5406.pt\" \n",
    "\n",
    "model_1 = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 예측하고자 하는 클래스의 수를 넣어줘야함\n",
    "model_1.load_state_dict(torch.load(model_path))\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_1.to(device)\n",
    "\n",
    "models_list.append((model_name_1, model_1, tokenizer_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"klue/roberta-base\"  # 허깅페이스에서 모델만 갈아끼우면 됨\n",
    "model_name_1 = \"klue_roberta-base_Fill_mask_cross_entropy_stopword_classification_model_3.pt\"  # 허깅페이스에서 모델만 갈아끼우면 됨\n",
    "model_path = os.getenv('HOME') + \"/aiffel/project/FnGuide/model/klue_roberta-base_Fill_mask_cross_entropy/klue_roberta-base_Fill_mask_cross_entropy_stopword_classification_model_3.pt\" \n",
    "\n",
    "model_2 = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 예측하고자 하는 클래스의 수를 넣어줘야함\n",
    "model_2.load_state_dict(torch.load(model_path))\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_2.to(device)\n",
    "\n",
    "models_list.append((model_name_1 ,model_2, tokenizer_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fbf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"klue/roberta-base\"  # 허깅페이스에서 모델만 갈아끼우면 됨\n",
    "model_name='kfdeberta-base_mix_document'\n",
    "p_model_path = os.getenv('HOME') + \"/aiffel/project/FnGuide/kfdeberta-base\"\n",
    "model_path = os.getenv('HOME') + \"/aiffel/project/FnGuide/model/kfdeberta-base_mix_document_cross_entropy/kfdeberta-base_mix_document_cross_entropy_stopword_classification_model_3.pt\" \n",
    "\n",
    "\n",
    "# model_3 = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 예측하고자 하는 클래스의 수를 넣어줘야함\n",
    "model_3 = AutoModelForSequenceClassification.from_pretrained(p_model_path, num_labels=2)\n",
    "model_3.load_state_dict(torch.load(model_path))\n",
    "# tokenizer_3 = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_3 = AutoTokenizer.from_pretrained(p_model_path)\n",
    "\n",
    "model_3.to(device)\n",
    "\n",
    "models_list.append((model_name,model_3, tokenizer_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b0c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"klue/roberta-base\"  # 허깅페이스에서 모델만 갈아끼우면 됨\n",
    "model_name_1='klue_roberta-base_summarize+Koeda_cross_entropy_stopword_classification_model_3.pt'\n",
    "p_model_path = os.getenv('HOME') + \"/aiffel/project/FnGuide/kfdeberta-base\"\n",
    "model_path = os.getenv('HOME') + \"/aiffel/project/FnGuide/model/klue_roberta-base_summarize+Koeda_cross_entropy/klue_roberta-base_summarize+Koeda_cross_entropy_stopword_classification_model_3.pt\" \n",
    "\n",
    "\n",
    "model_4 = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 예측하고자 하는 클래스의 수를 넣어줘야함\n",
    "# model_4 = AutoModelForSequenceClassification.from_pretrained(p_model_path, num_labels=2)\n",
    "model_4.load_state_dict(torch.load(model_path))\n",
    "tokenizer_4 = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer_4 = AutoTokenizer.from_pretrained(p_model_path)\n",
    "\n",
    "model_4.to(device)\n",
    "\n",
    "models_list.append((model_name_1,model_4, tokenizer_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7501e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=True, train=True, tokenizer=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:  # train data이면 sentence와 label 같이 전처리\n",
    "            labels = self.data['label'][index]\n",
    "            sentence = self.data['document'][index]\n",
    "        else:  # test data이면 label만 같이 전처리\n",
    "            sentence = self.data['document'][index]\n",
    "\n",
    "        encoded_dict = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "#             return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        padded_token_list = encoded_dict['input_ids'][0]\n",
    "        token_type_id = encoded_dict['token_type_ids'][0]\n",
    "        att_mask = encoded_dict['attention_mask'][0]\n",
    "\n",
    "        if self.train:\n",
    "            target = torch.tensor(labels)\n",
    "            sample = (padded_token_list, token_type_id, att_mask, target)\n",
    "        else:\n",
    "            sample = (padded_token_list, token_type_id, att_mask)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e98a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 모델 앙상블로부터 얻은 각 모델들의 예측 리스트\n",
    "preds_list = []\n",
    "\n",
    "# 각 모델로부터 예측 수행\n",
    "for model_name, model, tokenizer in models_list:\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    test_dataset = TrainTestDataset(test_df, train=False, tokenizer=tokenizer)  # 모델에 맞는 데이터로더 생성\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                  batch_size=8,\n",
    "                                                  shuffle=False,\n",
    "                                                  num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids_batch, token_type_id_batch, attention_masks_batch in tqdm(test_dataloader):\n",
    "            y_pred = model(input_ids_batch.to(device), token_type_ids=token_type_id_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "            _, predicted = torch.max(y_pred, 1)\n",
    "            pred.extend(predicted.tolist())\n",
    "            \n",
    "        print(\"model name : \", model_name)\n",
    "        accuracy = accuracy_score(pred, test_df['label'])\n",
    "        print(accuracy_score(pred, test_df['label']))\n",
    "        classification_report_str = classification_report(test_df['label'], pred, target_names=['real_news', 'AD'])\n",
    "        print(classification_report(test_df['label'], pred, target_names=['real_news', 'AD']))\n",
    "        \n",
    "        cm = confusion_matrix(test_df['label'], pred)\n",
    "        sns.heatmap(cm, annot = True, cmap='coolwarm', xticklabels=['real_news', 'AD'], yticklabels=['real_news', 'AD'], fmt='d')\n",
    "        plt.show()\n",
    "    \n",
    "    preds_list.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0624a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 보팅 앙상블로부터 얻은 앙상블 예측 리스트\n",
    "ensemble_pred = []\n",
    "\n",
    "# 각 샘플 별로 보팅 앙상블 적용\n",
    "for idx in range(len(test_dataset)):\n",
    "    votes = [preds[idx] for preds in preds_list]\n",
    "    majority_vote = Counter(votes).most_common(1)[0][0]\n",
    "    ensemble_pred.append(majority_vote)\n",
    "\n",
    "# 앙상블 결과 출력\n",
    "accuracy = accuracy_score(ensemble_pred, test_df['label'])\n",
    "print(\"Ensemble Accuracy:\", accuracy)\n",
    "classification_report_str = classification_report(test_df['label'], ensemble_pred, target_names=['real_news', 'AD'])\n",
    "print(classification_report(test_df['label'], ensemble_pred, target_names=['real_news', 'AD']))\n",
    "cm = confusion_matrix(test_df['label'], ensemble_pred)\n",
    "sns.heatmap(cm, annot = True, cmap='coolwarm', xticklabels=['real_news', 'AD'], yticklabels=['real_news', 'AD'], fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e61f1",
   "metadata": {},
   "source": [
    "## 5-3. Captum(어텐션 맵)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7209c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"klue/roberta-base\"  # 허깅페이스에서 모델만 갈아끼우면 됨\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 예측하고자 하는 클래스의 수를 넣어줘야함\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 학습시킨 모델 불러오기\n",
    "model_save_folder = os.getenv('HOME') + \"/aiffel/project/FnGuide/model\"\n",
    "model_path = os.path.join(model_save_folder, \"klue_roberta-base_None_cross_entropy_stopword_classification_model_3.pt\")\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"klue/roberta-base\", output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c464af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 문장 준비\n",
    "input_text = \"\"\"만도는 올 2분기 연결기준 영업손실이 758억 원으로 전년 동기 대비 246. 5% 감소했다고 30일 공시했다.\"\"\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# 문장을 토큰화하여 인덱스로 변환하고, 토큰들을 리스트로 저장합니다.\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 평가 상태로 변경\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # base_model의 forward 함수를 호출하여 결과를 얻습니다.\n",
    "    # output_attentions=True로 설정하여 어텐션 가중치를 가져옵니다.\n",
    "    outputs = base_model(input_ids=input_ids, output_attentions=True)\n",
    "    \n",
    "    # 어텐션 가중치를 변수 attentions에 저장합니다.\n",
    "    attentions = outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813d43a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_token2token_scores(attentions: list, layer_idx: int):\n",
    "    \"\"\"\n",
    "    어텐션 가중치 시각화 함수\n",
    "    Input:\n",
    "        attentions (List[Tensor]): 레이어별 어텐션 가중치가 담긴 리스트\n",
    "        layer_idx (int): 시각화하고자 하는 레이어 인덱스\n",
    "\n",
    "    Returns:\n",
    "        None: 어텐션 가중치를 시각화하여 표시합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    # layer_idx번째 레이어에 해당하는 어텐션 가중치의 shape를 확인하여 머리의 개수를 얻습니다.\n",
    "    num_heads = attentions[layer_idx].shape[1]\n",
    "\n",
    "    fig, axes = plt.subplots(num_heads, 1, figsize=(30, 90))\n",
    "\n",
    "    for head_idx in range(num_heads):\n",
    "        # layer_idx번째 레이어의 head_idx번째 머리의 어텐션 가중치를 가져옵니다.\n",
    "        scores_np = attentions[layer_idx][0, head_idx].detach().cpu().numpy()\n",
    "        \n",
    "        # subplot을 선택합니다. (만약 num_heads가 1이면 하나의 subplot만 사용합니다)\n",
    "        ax = axes[head_idx] if num_heads > 1 else axes\n",
    "        \n",
    "        # 어텐션 가중치를 이미지 형태로 그립니다.\n",
    "        im = ax.matshow(scores_np, cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(all_tokens)))\n",
    "        ax.set_yticks(range(len(all_tokens)))\n",
    "\n",
    "        ax.set_xticklabels(all_tokens, fontdict=fontdict, rotation=90)\n",
    "        ax.set_yticklabels(all_tokens, fontdict=fontdict)\n",
    "        ax.set_xlabel('Layer {}, Head {}'.format(layer_idx + 1, head_idx + 1))\n",
    "\n",
    "        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 0 # 보고싶은 레이어\n",
    "visualize_token2token_scores(attentions, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152b783c",
   "metadata": {},
   "source": [
    "## 5-4. Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9782d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(aug_data: pd.Dataframe, aug_back_data: pd.Dataframe) -> float:\n",
    "    \"\"\"\n",
    "    Multi 증강 방법에서 perplexity 비교\n",
    "    \n",
    "    Input:\n",
    "        aug_data (pd.Dataframe): 하나의 증강법을 적용한 데이터프레임\n",
    "        aug_back_data (pd.Dataframe): Multi 증강법을 적용한 데이터프레임\n",
    "        \n",
    "    Output:\n",
    "        aug_ppl_avg (float) : 하나의 증강법의 데이터들에 대한 perplexity score\n",
    "        aug_back_ppl_avg (float) : Multi 증강법의 데이터들에 대한 perplexity score\n",
    "    \"\"\"\n",
    "    aug_ppl = []\n",
    "    aug_back_ppl = []\n",
    "    \n",
    "    aug_list = aug_data['document'].tolist()\n",
    "    aug_back_list = aug_back_data['document'].tolist()\n",
    "    \n",
    "    for text in aug_list:\n",
    "        predictions = [text]\n",
    "        perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "        results = perplexity.compute(predictions=predictions, model_id='skt/kogpt2-base-v2', add_start_token=False)\n",
    "        aug_ppl.append(results['perplexities'][0])\n",
    "\n",
    "    for text in aug_back_list:\n",
    "        predictions = [text]\n",
    "        perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "        results = perplexity.compute(predictions=predictions, model_id='skt/kogpt2-base-v2', add_start_token=False)\n",
    "        aug_back_ppl.append(results['perplexities'][0])\n",
    "    \n",
    "    aug_ppl_avg = sum(aug_ppl) / len(aug_ppl)\n",
    "    aug_back_ppl_avg = sum(aug_back_ppl) / len(aug_back_ppl)\n",
    "    \n",
    "    return aug_ppl_avg, aug_back_ppl_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387aee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df_1 = mix_document(data=train_df, label=1, num=10, min_blocks = min_blocks, mix_ratio_range=(0.2, 0.4), mode='kss', shuffle_mode=2)\n",
    "augmented_df = back_translate(data=augmented_df_1, label=1, num=10, mode=\"back_translate\", target_language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c00193",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_ppl_avg, mix_back_ppl_avg = perplexity(augmented_df_1, augmented_df)\n",
    "print(\"mix_ppl_avg:\", mix_ppl_avg)\n",
    "print(\"mix_back_ppl_avg:\", mix_back_ppl_avg)\n",
    "\n",
    "\"\"\"\n",
    "OUTPUT:\n",
    "mix_ppl_avg: 64.56231307983398\n",
    "mix_back_ppl_avg: 42.96618232727051\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
