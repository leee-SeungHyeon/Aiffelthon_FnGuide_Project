{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69b6034",
   "metadata": {},
   "source": [
    "# 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416d78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset,TensorDataset\n",
    "from torch.nn import functional as F\n",
    "import re\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from konlpy.tag import Mecab\n",
    "import wandb\n",
    "import collections\n",
    "import pickle\n",
    "import kss\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from transformers import set_seed\n",
    "\n",
    "from ktextaug import TextAugmentation\n",
    "import ktextaug\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelForMaskedLM\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.family'] = 'NanumGothic'\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "set_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(random_seed)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad37ed2b",
   "metadata": {},
   "source": [
    "# 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21452df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):#파일이름만 넣기\n",
    "    data_path = os.getenv('HOME') + '/aiffel/project/FnGuide/data/ad_news'#경로는 자신의 경로로 바꿔서 하면 됨\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    df = pd.read_csv(file_path, encoding='utf-8', delimiter='\\t')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec294c",
   "metadata": {},
   "source": [
    "## 1-1 사전학습 모델 및 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dfa548a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"klue/roberta-base\"  # 허깅페이스에서 모델만 갈아끼우면 됨\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 예측하고자 하는 클래스의 수를 넣어줘야함\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40f5e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /aiffel/aiffel/project/FnGuide/kfdeberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# kb-albert-char-base-v2, kfdeberta-base\n",
    "model_name='kfdeberta-base'\n",
    "\n",
    "p_model_path = os.getenv('HOME') + \"/aiffel/project/FnGuide/kfdeberta-base\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(p_model_path, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(p_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d1737",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1b755",
   "metadata": {},
   "source": [
    "# 3. 3-1Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3769cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data('preprocess_train.csv')\n",
    "val_df = load_data('preprocess_val.csv')\n",
    "test_df = load_data('preprocess_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d992a072",
   "metadata": {},
   "source": [
    "### 3-1-1. Back translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba7d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate(data: pd.DataFrame, label: int, num: int, mode: str=\"back_translate\", target_language: str='en', prob: int=0.4)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            data (pandas.DataFrame) : CSV 파일로 구성된 데이터\n",
    "            label (int) : 증강에 사용할 레이블\n",
    "            num (int) : 증강 개수\n",
    "            mode (str, optional) : 데이터 증강 모드. (back_translate/noise_add), 기본값은 \"back_translate\"\n",
    "            target_language (str, optional) : 번역할 언어. 기본값은 'en' (영어)\n",
    "            prob (float, optional) : noise_add 모드에서의 적용 확률\n",
    "        \n",
    "        Returns:\n",
    "            augmented_df (pd.DataFrame) : 증강된 데이터들로 구성된 DataFrame  \n",
    "    \"\"\"\n",
    "    \n",
    "    # TextAugmentation 객체 생성\n",
    "    agent = TextAugmentation(tokenizer=\"mecab\", num_processes=1)\n",
    "\n",
    "    # 선택한 레이블로 데이터 필터링\n",
    "    filtered_data = data[data['label'] == label]['document'].tolist()\n",
    "\n",
    "    # 데이터 증강\n",
    "    augmented_data = []\n",
    "    selected_data_set = set()  # 중복 선택된 데이터를 추적하기 위한 집합\n",
    "\n",
    "    for _ in range(num):\n",
    "        selected_data = random.choice(filtered_data)  # 원본 데이터 중에서 무작위로 선택\n",
    "\n",
    "        while selected_data in selected_data_set:\n",
    "            selected_data = random.choice(filtered_data)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "        selected_data_set.add(selected_data)  # 선택된 데이터를 집합에 추가\n",
    "\n",
    "        # 데이터 증강\n",
    "        if mode == \"noise_add\":\n",
    "            generated_data = agent.generate(selected_data, mode=\"noise_add\", prob=prob, noise_mode=['phonological_change', 'vowel_change', 'jamo_split'])\n",
    "        else:\n",
    "            generated_data = agent.generate(selected_data, mode=\"back_translate\", target_language=target_language)  #일본어로 하려면 target_language='jp'\n",
    "        \n",
    "        augmented_data.append(generated_data)\n",
    "\n",
    "    # 생성된 데이터를 DataFrame으로 변환\n",
    "    augmented_df = pd.DataFrame({'document': augmented_data, 'label': label})\n",
    "    \n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806f622",
   "metadata": {},
   "source": [
    "### 3-1-2. Document Mixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1c9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_blocks = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01ae76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text: str, mode: str ='kss') -> list :\n",
    "    \"\"\"\n",
    "    문장을 분리하는 함수\n",
    "    \n",
    "    Input:\n",
    "        text (str): 증강할 문장\n",
    "        mode (str): 문장을 분리하는 방식\n",
    "\n",
    "    Output:\n",
    "        sentences(list): 분리된 문장 리스트\n",
    "    \"\"\"\n",
    "    if mode == 'to':\n",
    "        sentences = tokenizer.tokenize(text)  # 토크나이저를 사용하여 문장 단위로 분리\n",
    "\n",
    "    else:\n",
    "        sentences = kss.split_sentences(text)  # Split sentences using kss library\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3324f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_shuffle(text: str, min_blocks: int = min_blocks) -> str:\n",
    "    \"\"\"\n",
    "    하나의 문장내에서 문장 순서를 뒤섞는 함수\n",
    "    \n",
    "    Input:\n",
    "        text (str): 증강할 문장\n",
    "        min_blocks (str): 최소 문장 수\n",
    "\n",
    "    Output:\n",
    "        augmented_text (str): 셔플된 하나의 텍스트\n",
    "    \"\"\"\n",
    "    sentences = tokenize_sentences(text, 'kss') \n",
    "    \n",
    "    # 분리된 문장들 중에서 하나 이상의 문장 블록을 선택하기 위해 최소 블록 개수를 설정\n",
    "    min_blocks = min(min_blocks, len(sentences))\n",
    "    \n",
    "    # 무작위로 두 개 이상의 문장 블록을 선택\n",
    "    num_blocks_to_shuffle = random.randint(min_blocks, len(sentences)) #min_blocks부터 len(sentences) 사이의 정수 에서 무작위로 하나의 정수를 반환하는 함수\n",
    "    selected_blocks_indices = random.sample(range(len(sentences)), num_blocks_to_shuffle)\n",
    "    \n",
    "    # 선택된 문장 블록들의 위치를 서로 바꿈\n",
    "    shuffled_sentences = [sentences[i] for i in range(len(sentences)) if i not in selected_blocks_indices]\n",
    "    for i in selected_blocks_indices:\n",
    "        shuffled_sentences.append(sentences[i])\n",
    "    \n",
    "    # 문장 블록들을 다시 하나의 문자열로 합쳐서 증강된 문장을 생성\n",
    "    augmented_text = ' '.join(shuffled_sentences)\n",
    "    \n",
    "    return augmented_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4d848db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_document(data, label, num, min_blocks = min_blocks, mix_ratio_range=(0.2, 0.4), mode='kss', shuffle_mode=2)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            data (pandas.DataFrame) : CSV 파일로 구성된 데이터\n",
    "            label (int) : 증강에 사용할 레이블\n",
    "            num (int) : 증강 개수\n",
    "            min_blocks (str): 최소 문장 수\n",
    "            mix_ratio_range (tuple) : 데이터에서 사용할 문장 선택 비율, 기본값 0.2~0.4에서 랜덤\n",
    "            mode (str, optional) : 데이터 증강 모드. (kss/tokenize), 기본값은 \"back_translate\"\n",
    "            shuffle_mode (int) : 문장 mix할 개수, (1,2), 기본값은 2\n",
    "        \n",
    "        Returns:\n",
    "            augmented_df (pd.DataFrame) : 증강된 데이터들로 구성된 DataFrame  \n",
    "    \"\"\"\n",
    "    augmented_data = []\n",
    "    filtered_data = data[data['label'] == label]['document'].tolist()\n",
    "    \n",
    "    selected_data_set = set()  # 중복 선택된 쌍을 추적하기 위한 집합\n",
    "    \n",
    "    for _ in range(num):\n",
    "        \n",
    "        if shuffle_mode == 2:\n",
    "        \n",
    "            selected_data = random.sample(filtered_data, 2)  # 원본 데이터 중에서 2개를 무작위로 선택\n",
    "\n",
    "            while tuple(selected_data) in selected_data_set:\n",
    "                selected_data = random.sample(filtered_data, 2)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "            selected_data_set.add(tuple(selected_data))\n",
    "\n",
    "            sentences_1 = tokenize_sentences(selected_data[0], mode)  # 첫 번째 선택된 데이터의 문장으로 분리\n",
    "            sentences_2 = tokenize_sentences(selected_data[1], mode)  # 두 번째 선택된 데이터의 문장으로 분리\n",
    "\n",
    "            mix_ratio_1 = random.uniform(*mix_ratio_range)\n",
    "            mix_ratio_2 = random.uniform(*mix_ratio_range)\n",
    "\n",
    "            # sentences_1에서 특정 비율만큼 문장 제거\n",
    "            num_sentences_to_remove = int(len(sentences_1) * mix_ratio_1)\n",
    "            removed_sentences = random.sample(sentences_1, num_sentences_to_remove)\n",
    "            augmented_sentences_1 = [sentence for sentence in sentences_1 if sentence not in removed_sentences]\n",
    "\n",
    "\n",
    "            # sentences_2에서 특정 비율만큼 문장 추출\n",
    "            num_sentences_to_extract_2 = int(len(sentences_2) * mix_ratio_2)\n",
    "            extracted_sentences_2 = random.sample(sentences_2, num_sentences_to_extract_2)\n",
    "\n",
    "            # 추출된 문장들을 랜덤하게 위치에 삽입하여 augmented_sentences_1에 삽입\n",
    "            for sentence in extracted_sentences_2:\n",
    "                random_index = random.randint(0, len(augmented_sentences_1))\n",
    "                augmented_sentences_1.insert(random_index, sentence)\n",
    "\n",
    "            if mode == 'to':\n",
    "                augmented_text = tokenizer.convert_tokens_to_string(augmented_sentences_1)  # 토큰을 다시 문자열로 변환하여 문서로 만듦\n",
    "            else:\n",
    "                augmented_text = ' '.join(augmented_sentences_1)  # 문장 내의 단어들을 공백으로 연결하여 문장으로 만듦\n",
    "\n",
    "            augmented_data.append(augmented_text)\n",
    "            \n",
    "        elif shuffle_mode == 1:\n",
    "        \n",
    "            selected_data = random.choice(filtered_data)# 원본 데이터 중에서 1개를 무작위로 선택\n",
    "\n",
    "            while tuple(selected_data) in selected_data_set:\n",
    "                selected_data = random.choice(filtered_data)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "            selected_data_set.add(tuple(selected_data))\n",
    "\n",
    "            augmented_text = sentence_shuffle(selected_data, min_blocks)\n",
    "            augmented_data.append(augmented_text)\n",
    "        \n",
    "    # 생성된 증강 데이터에 레이블 1 추가\n",
    "    augmented_df = pd.DataFrame({'document': augmented_data, 'label': label})\n",
    "        \n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7a0f0",
   "metadata": {},
   "source": [
    "### 3-1-3. KoEDA\n",
    "\n",
    "p = (alpha_sr, alpha_ri, alpha_rs, prob_rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75652d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from koeda import EDA\n",
    "\n",
    "def ag_koeda(data: pd.DataFrame, label: int, num: int, p: tuple=(0.3, 0.3, 0.3, 0.3))-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        data (DataFrame): 증강시킬 데이터가 있는 데이터 프레임\n",
    "        label (int): 증강시키고 싶은 label\n",
    "        num (int): 증강시킬 데이터의 수\n",
    "        p (tuple): alpha_sr, alpha_ri, alpha_rs, prob_rd 기법의 수치\n",
    "        \n",
    "    Output:\n",
    "        augmented_df (DataFrame): 증강시킨 데이터가 있는 데이터 프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    eda = EDA(morpheme_analyzer=\"Mecab\", alpha_sr=p[0], alpha_ri=p[1], alpha_rs=p[2], prob_rd=p[3])\n",
    "\n",
    "    # 데이터 필터링\n",
    "    filtered_data = data[data['label'] == label]['document'].tolist()\n",
    "    \n",
    "    selected_data_set = set()  # 중복 선택된 데이터를 추적하기 위한 집합\n",
    "\n",
    "    # 데이터 증강\n",
    "    augmented_data = []\n",
    "    for _ in range(num):\n",
    "        selected_data = random.choice(filtered_data)  # 원본 데이터 중에서 무작위로 선택\n",
    "\n",
    "        while selected_data in selected_data_set:\n",
    "            selected_data = random.choice(filtered_data)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "        selected_data_set.add(selected_data)  # 선택된 데이터를 집합에 추가\n",
    "\n",
    "        # 데이터 증강\n",
    "        generated_data = eda(selected_data, p=p, repetition=1)  # repetition: 한 문장을 몇 개로 증강시킬 것인지\n",
    "        augmented_data.append(generated_data)\n",
    "\n",
    "    # 생성된 데이터를 DataFrame으로 변환\n",
    "    augmented_df = pd.DataFrame({'document': augmented_data, 'label': label})\n",
    "\n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6c1161",
   "metadata": {},
   "source": [
    "### 3-1-4. Fill Masked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65b979fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_masked_augmentation(df: pd.DataFrame, label_num: int, random_number: int, num_masks_ratio: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        df (DataFrame): 증강시킬 데이터가 있는 데이터 프레임\n",
    "        label_num (int): 증강시키고 싶은 label number\n",
    "        random_number (int): 증강시킬 데이터의 수\n",
    "        num_masks_ratio (float): 본문에서 마스킹 시킬 비율\n",
    "        max_length (int): sentence 토큰의 최대 길이\n",
    "        \n",
    "    Output:\n",
    "        aug_data (DataFrame): 증강시킨 데이터가 있는 데이터 프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    label_data = df[df['label'] == label_num]\n",
    "    predicted_tokens_list = []\n",
    "    fill_model.to(device)\n",
    "    random.seed(random_seed) #이렇게 하니까 증강문이 고정됨(마스킹도 고정)\n",
    "    \n",
    "    for _ in tqdm(range(random_number)):\n",
    "        \n",
    "        random_sentences = random.choice(label_data['document'].tolist()) \n",
    "\n",
    "        num_masks = int(len(random_sentences.split()) * num_masks_ratio)\n",
    "        num_masks = min(num_masks, 20)\n",
    "        words = random_sentences.split()\n",
    "        masked_indices = random.sample(range(len(words)), num_masks)\n",
    "        masked_text = ' '.join('[MASK]' if i in masked_indices else word for i, word in enumerate(words))\n",
    "        tokenized_sentence = fill_tokenizer.tokenize(masked_text)\n",
    "        masked_indices = [i for i, token in enumerate(tokenized_sentence) if token == fill_tokenizer.mask_token]\n",
    "        \n",
    "        predicted_tokens = []\n",
    "        for index in masked_indices:\n",
    "            tokens = tokenized_sentence.copy()\n",
    "            tokens[index] = fill_tokenizer.mask_token\n",
    "            input_ids = fill_tokenizer.convert_tokens_to_ids(tokens)\n",
    "            inputs = torch.tensor([input_ids]).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = fill_model(inputs)\n",
    "                predictions = outputs.logits[0, index].topk(k=5)\n",
    "            \n",
    "            max_count = 5\n",
    "            count = 0\n",
    "            random_prediction_index = torch.multinomial(predictions.values, 1)  # 상위 5개 중에서 랜덤하게 하나 선택\n",
    "            random_prediction_token_id = predictions.indices[random_prediction_index].item()  # 선택한 토큰 ID 추출\n",
    "            random_prediction_token = fill_tokenizer.convert_ids_to_tokens([random_prediction_token_id])[0]  # 토큰으로 변환\n",
    "            \n",
    "            while random_prediction_token in [\"(\",\")\",\" \",\".\", \",\", \"'\", '\"',\"‘\",\"’\",'“','”',\"[SEP]\", \"[PAD]\", \"[UNK]\"]:\n",
    "                count += 1\n",
    "                if count >= max_count:\n",
    "                    random_prediction_token = \" \"\n",
    "                    break\n",
    "                random_prediction_index = torch.multinomial(predictions.values, 1)  # 상위 5개 중에서 랜덤하게 하나 선택\n",
    "                random_prediction_token_id = predictions.indices[random_prediction_index].item()  # 선택한 토큰 ID 추출\n",
    "                random_prediction_token = fill_tokenizer.convert_ids_to_tokens([random_prediction_token_id])[0]  # 토큰으로 변환\n",
    "                \n",
    "            predicted_tokens.append(random_prediction_token)\n",
    "\n",
    "        new_sentence = tokenized_sentence.copy()\n",
    "\n",
    "        for index, predicted_token in zip(masked_indices, predicted_tokens):\n",
    "            new_sentence[index] = predicted_token\n",
    "\n",
    "        new_sentence = fill_tokenizer.convert_tokens_to_string(new_sentence).strip()\n",
    "        predicted_tokens_list.append(new_sentence)\n",
    "\n",
    "    aug_data = pd.DataFrame({'document': predicted_tokens_list, 'label': label_num})\n",
    "\n",
    "    return aug_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4eb84",
   "metadata": {},
   "source": [
    "### 3-1-5. Summarize sentences\n",
    "#### 3-1-5-1. 추출적 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf2783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_function(text: str, summarize_ratio: float) -> str:\n",
    "    '''\n",
    "    텍스트를 요약시키는 함수\n",
    "    \n",
    "    Input:\n",
    "        text (str): 요약시킬 텍스트\n",
    "        summarize_ratio (float): 요약 비율\n",
    "       \n",
    "    Output:\n",
    "        summraize_text (str): 요약된 텍스트    \n",
    "    '''\n",
    "    summraize_text = summarize(text, ratio = summarize_ratio)\n",
    "    summraize_text = summraize_text.replace('\\n', ' ')\n",
    "    \n",
    "    return summraize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3fae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_summarize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    임베딩된 텍스트의 길이가 512가 넘으면 텍스트를 요약하여 요약된 데이터를 이용한 데이터프레임을 만드는 함수\n",
    "    \n",
    "    Input:\n",
    "        df (DataFrame): 요약시킬 데이터가 있는 데이터프레임\n",
    "       \n",
    "    Output:\n",
    "        augmented_df (DataFrame): 요약된 텍스트가 있는 데이터프레임  \n",
    "    '''\n",
    "    \n",
    "    summarize_data = []\n",
    "    labels = []\n",
    "    index = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        if 512 < len(tokenizer.encode(df['document'][i])):\n",
    "            aa = summarize_function(df['document'][i], 0.5)\n",
    "            summarize_data.append(aa)\n",
    "            labels.append(df['label'][i])\n",
    "            index.append(i)\n",
    "    augmented_df = pd.DataFrame({'document': summarize_data, 'label': labels, 'index': index})\n",
    "    \n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8aca9",
   "metadata": {},
   "source": [
    "#### 3-1-5-2. 추상적 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e8de89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230382a4ef1e4a9abb4a9c4a1b944a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644210ab803f4fcaa86c32aaa49dc489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a259888c8686429ea198255fea6930db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3193678cfc4b67b21d3b0622a382e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66bc7fc0e1104e8fb8e90b2a189c4a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/666k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "class KoBARTConditionalGeneration(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KoBARTConditionalGeneration, self).__init__()\n",
    "        self.model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v2')\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        attention_mask = inputs['input_ids'].ne(self.pad_token_id).float()\n",
    "        decoder_attention_mask = inputs['decoder_input_ids'].ne(self.pad_token_id).float()\n",
    "\n",
    "        return self.model(input_ids=inputs['input_ids'],\n",
    "                          attention_mask=attention_mask,\n",
    "                          decoder_input_ids=inputs['decoder_input_ids'],\n",
    "                          decoder_attention_mask=decoder_attention_mask,\n",
    "                          labels=inputs['labels'], return_dict=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = KoBARTConditionalGeneration().to(device)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99594f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성\n",
    "model_wrapper = KoBARTConditionalGeneration().to(device)\n",
    "\n",
    "# 가중치 로드\n",
    "model_path_gen = os.getenv('HOME') + \"/aiffel/project/FnGuide/model/best_model.pt\"\n",
    "model_wrapper.load_state_dict(torch.load(model_path_gen))\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model_wrapper.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d05cbfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sentences(sentences, ratio):\n",
    "    num_groups = len(sentences) // ratio\n",
    "    grouped_sentences = [''.join(sentences[i : i + ratio]) for i in range(0, len(sentences), ratio)]\n",
    "    return grouped_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f4e842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰기준 512넘어가는 데이터 레이블 리스트\n",
    "\n",
    "file_name = '512_over_text_index.txt'\n",
    "\n",
    "with open(file_name, 'r') as file:\n",
    "    list_str = file.read()\n",
    "\n",
    "gen_index_list = [int(item) for item in list_str.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fb70416",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'삼성은 삼성전자와 계열사 임직원의 글로벌 역량을 총동원해 2030 부산세계박람회 유치 활동에 나서고 . 부산엑스포 유치를 위해 대한민국 대표 기업으로서 솔선수범한다는 자세로 총력전을 펼친다는 계획이다 . 임직원 세계 돌며 유치 홍보삼성전자는 주요 경영진을 주축으로 세계박람회기구 회원국 관계자들을 직접 만나 유치 활동을 지원하고 . 한종희 삼성전자 디바이스경험부문장은 최근 스웨덴 , 남아프리카공화국 등을 방문해 현지 사업을 점검하고 정부 관계자들을 만나 부산엑스포 지지를 요청했다 . 8월 15일엔 스웨덴 스톡홀름에서 안나 할베리 통상장관을 접견해 스웨덴 정부의 주요 관심사인 순환 경제 녹색 전환 대해 논의했다 . 같은 달 11일에는 남아프리카공화국 프리토리아에서 이브라힘 파텔 통상산업부 장관과 날레디 판도르 국제협력부 장관을 만났다 . 노태문 모바일경험사업부장은 8월 5일 베트남 하노이에서 팜 민 찐 베트남 총리를 접견해 부산엑스포 지지를 요청했다 . 16일에는 파나마 파나마시티에서 삼성전자 청소년 기술교육 프로그램인 삼성이노베이션캠퍼스 행사에 참석해 야스민 콜론 데 코르티소 영부인 , 페데리코 알파로 보이드 통상산업부 장관을 접견했다 . 이재승 생활가전사업부장은 엘살바도르 , 코스타리카 , 도미니카공화국 중남미 3개국을 방문해 현지 사업 현황을 점검하고 각국 관계자를 만나 부산엑스포 유치 지지를 요청했다 . 이인용 cr담당 사장은 8월 19일 동티모르 딜리에서 조제 하무스 오르타 동티모르 대통령과 타우르 마탄 루아크 총리를 만나 삼성전자의 혁신 기술 사례와 사회공헌 활동을 설명하고 부산엑스포 협력을 요청했다 . 국가 간 주요 행사에 맞춰 부산엑스포 유치 활동을 알리기도 했다 . 박학규 경영지원실장은 7월 피지 수바에서 열린 태평양도서국포럼 정상회의 맞아 현지에서 참가국 관계자들과 면담하고 삼성전자의 혁신 기술 사례와 사회공헌 활동을 설명하며 태평양 도서국들의 부산엑스포 유치 협력을 당부했다 . 해외 곳곳 옥외광고 홍보도삼성전자는 이와 해외 주요 장소에 부산엑스포 유치 홍보 광고를 진행해 왔다 . 7월 피지 pif 정상회의 때에는 피지 시내와 주요 공항에서 부산엑스포 유치를 응원하는 옥외광고를 상영했다 . 동티모르의 수도 딜리 , 남아프리카공화국의 요하네스버그 , 센추리온 주요 도시에서도 옥외 광고를 하고 . 7월 캄보디아 , 라오스에서 열린 한국대사관 주최 태권도대회에서도 부산엑스포 유치를 기원하는 홍보 활동을 했다 . 삼성전자는 대회장에 부산엑스포를 알리는 광고물을 설치하고 안내 책자를 비치해 참가자들에게 부산엑스포를 소개했다 . 대회 현장에서 삼성 스마트폰과 tv 제품 체험존을 운영하고 부산엑스포 홍보 영상을 상영했다 . 국내에서 홍보 활동도 다양하게 진행됐다 . 엑스포 개최 후보 도시인 부산 지역에 이어 전국으로 범위를 넓혀 매장 안팎 전시물과 사이니지 영상 등을 통해 부산엑스포 유치 응원 광고를 선보이고 . 스마트싱스 tv 광고 , 삼성 제품 신문 광고 등에서 유치 응원의 뜻을 이어가고 있고 , 새 제품이나 서비스를 선보일 광고에 엑스포 응원 문구를 넣어 홍보 활동을 지원한다는 방침이다 . 친환경 기술로 엑스포 유치 역량 높여삼성전자는 탄소 저감과 자원 순환 , 생태 복원 등의 활동을 통해 환경을 보호하고 인권과 다양성 존중 미래세대 교육 기술 혁신을 통한 포용적인 사회를 만들며 한국의 엑스포 유치 역량을 세계에 알리고 . 기후변화 대응을 위해 온실가스 감축 재생에너지 확대 제품 에너지 효율 향상에 힘쓰고 . 제조 공정에서의 온실가스 저감을 위해 공정 가스 처리 , 에너지 절감에 힘 쏟고 . 제조 공정의 효율성을 높이고 제조 설비 에너지를 절감해 온실가스 배출량 증가를 최소화하는 것이다 . 제조공정 에너지 절감을 위해 인프라 설비에 사물인터넷 기술을 적용했다 . iot , 인공지능 기반 공조 솔루션으로 에너지 사용량을 모니터링하고 효율적인 운전 제어를 구현해 연평균 11 13% 에너지를 절감했다 . 반도체 제조 공정의 전력 사용량을 절감하기 위해 공정 개선 과사용 전력 운전 최적화를 통한 메인 설비 테스트 단축 부대설비 운전 온도 조건 개선 고효율 설비 습식 스크러버 중성화 등을 적용했다 . 삼성전자는 올해 내놓은 휴대전화 주요 제품에 폐어망을 재활용한 소재를 사용하고 . 이를 통해 연내에 바다에 버려진 폐어망 50t을 수거해 재활용함으로써 해양 플라스틱이 끼치는 바다 생태계 위협을 줄이는 데 기여할 방침이다 . contributes to restoration of ecosystemwith advanced eco friendly technologysamsungexecutives of overseas business meet foreign high ranking officialsbriefs about cases of innovative technologies at major eventsprovides promotional videos in global experience zones for tv productssamsung group , led by globally well connected executives of samsung electronics and affiliated companies , is making concerted efforts to help busan win the bid to host the world expo 2030 busan . as south korea s representative corporation , samsung aims to take the lead in supporting the korean government s endeavors . executives actively promote south korea the leading management groups of samsung electronics are meeting with bie members to actively support busan in its attempt to win the bid . han jong hee , vice chairman of the device experience division , recently visited sweden and south africa to inspect local businesses , and met with government leaders to seek their support for the world expo 2030 busan . on august 15 , mr . han held an official meeting with anna hallberg , minister of foreign trade in stockholm , to discuss the circular economy and the green transition , which are policies sweden is actively pursuing . on the 11th of the same month , mr . han traveled to pretoria , one of south africa s three capitals , where he met with ebrahim patel , minister of trade , industry and competition , and grace naledi mandisa pandor , minister of international relations and cooperation . roh tae moon , president and head of the mobile experience division , met with pham minh chinh , prime minister of vietnam , in hanoi on august 5 to seek his support for the busan expo . on august 16 , mr . roh attended an event of the samsung innovation campus , education program of samsung electronics for youth , in panama city , panama and met with first lady yazmin colon de cortizo and federico alfaro boyd , the minister of commerce and industries . lee jae seung , who heads the digital appliances division , visited three countries in central america and south america , el salvador , costa rica , and the dominican republic , to inspect businesses on site and meet with business leaders to help support busan s successful bid to host the expo . rhee in yong , president of the corporate communication team , met with president of east timor jose ramos hort and prime minister jose taur matan ruak to explain examples of samsung electronics innovative technologies and social contribution activities and to seek support for busan to host the expo . samsung executives also rolled out promotional campaigns in time for major national events . arriving in time for the july pacific islands forum held in suva , the capital of fiji , park hark kyu , president and cfo of samsung electronics , met with the participants to explain samsung electronics innovative technologies and social contribution activities and to seek their support for the busan expo . outdoor advertising and promotionssamsung electronics has also rolled out advertising and promotion campaigns in major locations overseas . during the pif in july held in fiji , it installed outdoor advertising in city centers and major airports to support the world expo 2030 busan . the company also installed outdoor advertising in major cities , including dili , the capital of east timor , and johannesburg and centurion , south africa . in july at the taekwondo championships hosted by the korean embassy in cambodia and laos , samsung electronics organized promotional activities to support busan s bid to host the busan expo . at the venue , samsung electronics installed advertisements to publicize the busan expo and promotional booklets for the participants , operated the samsung zone to allow visitors to experience samsung smartphones and tvs , and screened promotional films about the busan expo . samsung electronics also launched a promotional campaign in south korea via advertising displays for indoor and outdoor stores and signage videos to support busan s bid to win the expo . the campaign started with busan , the expo venue , followed by the rest of the country , featuring samsung s support for the busan expo through smartthings ads on tv and samsung products in newspaper ads . when samsung electronics introduced new products or services , the company added special phrases to support the world expo 2030 busan . environmentally friendly technologies increase competencesamsung electronics aims to disseminate the south korea s competence in hosting the busan expo through the company s messages to the world , as samsung seeks to protect the environment through its efforts to reduce carbon emissions , circulate resources , restore the ecosystem , and help to build a more inclusive society by respecting human rights and diversity and providing education for future generations as well as technological innovations . responding to climate change , samsung is also dedicated to reducing greenhouse gas emissions , expanding renewable energy , and increasing product energy efficiency . to reduce greenhouse gas emissions during the manufacturing process , the company has made an effort to reduce consumption of process gas and energy in all its business processes . this will increase the efficiency of its manufacturing processes while decreasing the energy used by its machinery and facilities , thereby minimizing greenhouse gas emissions . samsung electronics has also applied iot technology to its infrastructure facilities to reduce the energy consumption of its manufacturing processes . iot and ai based mutual assistance solutions can monitor energy usage and implement efficient controls , reducing energy consumption by 11 13 percent per year on average . samsung electronics shortened the testing time required for its main facilities by improving the process and optimizing excess electric power to reduce the electricity consumption of the semiconductor manufacturing process . the company has improved the temperature controls used in subsidiary facilities , applied high efficiency facilities , and neutralized wet scrubbers . samsung electronics uses recycled materials , such as discarded fishing nets , in the manufacturing of its major products , including mobile phones . the company has so far recovered 50 tons of discarded fishing nets from the ocean and recycled them , thereby reducing the environmental threat to the marine ecosystem .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['document'][gen_index_list[233]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d0f51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(df: pd.DataFrame, gen_index_list : list) -> pd.DataFrame:\n",
    "    summarize_data = []\n",
    "    labels = []\n",
    "    index = []\n",
    "\n",
    "    for j in tqdm(range(len(gen_index_list))):\n",
    "        i = gen_index_list[j]\n",
    "        sentence = df['document'][i]\n",
    "        text = tokenize_sentences(sentence)\n",
    "        ratio = len(text) // 5\n",
    "        t_text = group_sentences(text, ratio)\n",
    "\n",
    "        p_text = []\n",
    "        for text in t_text:\n",
    "            input_ids = tokenizer.encode(text)\n",
    "            input_ids = torch.tensor(input_ids)\n",
    "            input_ids = input_ids.unsqueeze(0).to(device)\n",
    "            output = model_wrapper.model.generate(input_ids, eos_token_id=1, max_length=256, num_beams=10)\n",
    "            output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            p_text.append(output)\n",
    "\n",
    "        gen_text = ' '.join(p_text)\n",
    "        summarize_data.append(gen_text)\n",
    "        labels.append(df['label'][i])\n",
    "        index.append(i)\n",
    "            \n",
    "    augmented_df = pd.DataFrame({'document': summarize_data, 'label': labels, 'index': index})\n",
    "    \n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd20bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_aug = gen_text(train_df, gen_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강된것 저장\n",
    "gen_aug.to_csv(os.getenv('HOME') + '/aiffel/project/FnGuide/data/ad_news/gen_train.csv', index=False, encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train = pd.read_csv(os.getenv('HOME') + '/aiffel/project/FnGuide/data/ad_news/summarized_train.csv', sep='\\t')\n",
    "gen_train = gen_train[['document', 'label']] #이거 하고 해야 오류 안남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 증강된 것 포함해서 실험 진행\n",
    "gen_data = pd.concat([train_df, gen_train]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c9729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 증강된 것 대체해서 실험 진행\n",
    "index_list = gen_train.index.tolist()\n",
    "\n",
    "for i in gen_train['index']:\n",
    "    for j in index_list:\n",
    "        if i == j:\n",
    "            train_df['document'][j] = gen_train['document'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7beafc",
   "metadata": {},
   "source": [
    "## 3-2. 클래스 및 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d55cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=True, train=True):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:  # train data이면 sentence와 label 같이 전처리\n",
    "            labels = self.data['label'][index]\n",
    "            sentence = self.data['document'][index]\n",
    "        else:  # test data이면 label만 같이 전처리\n",
    "            sentence = self.data['document'][index]\n",
    "\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=config.MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "#             return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        padded_token_list = encoded_dict['input_ids'][0]\n",
    "        token_type_id = encoded_dict['token_type_ids'][0]\n",
    "        att_mask = encoded_dict['attention_mask'][0]\n",
    "\n",
    "        if self.train:\n",
    "            target = torch.tensor(labels)\n",
    "            sample = (padded_token_list, token_type_id, att_mask, target)\n",
    "        else:\n",
    "            sample = (padded_token_list, token_type_id, att_mask)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f03887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=3, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            \n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "324551cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_check(diff_indices, new_df):\n",
    "    for i in diff_indices:\n",
    "        print(\"인덱스:\", i)\n",
    "        print(\"Real news(%) : \", new_df['probability_negative'][i])\n",
    "        print(\"AD news(%) : \", new_df['probability_positive'][i])\n",
    "        print('실제 답변:{}, 예측 답변:{}'.format(new_df['label'][i],new_df['predicted_label'][i]))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c2911",
   "metadata": {},
   "source": [
    "# 4. 실험 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fbc296",
   "metadata": {},
   "source": [
    "#### 1. Back trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fb5928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = back_translate(data=train_df, label=1, num=1000, mode=\"back_translate\", target_language='en')\n",
    "train_back = pd.concat([train_df, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d62258",
   "metadata": {},
   "source": [
    "#### 2. mix document\n",
    "##### 2-1 문장 2개 mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7392331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Oh! You have mecab in your environment. Kss will take this as a backend! :D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_df_1 = mix_document(data=train_df, label=1, num=1000, min_blocks = min_blocks, mix_ratio_range=(0.2, 0.4), mode='kss', shuffle_mode=2)\n",
    "train_mix_1 = pd.concat([train_df, augmented_df_1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c9a6c",
   "metadata": {},
   "source": [
    "##### 2-2 문장 1개 mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e60ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df_2 = mix_document(data=train_df, label=1, num=1000, min_blocks = min_blocks, mix_ratio_range=(0.2, 0.4), mode='kss', shuffle_mode=1)\n",
    "train_mix_2 = pd.concat([train_df, augmented_df_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84dd1c",
   "metadata": {},
   "source": [
    "##### 2-3 문장 2개 + 1개 혼합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecb1e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df_3_1 = mix_document(data=train_df, label=1, num=500, min_blocks = min_blocks, mix_ratio_range=(0.2, 0.4), mode='kss', shuffle_mode=2)\n",
    "augmented_df_3_2 = mix_document(data=train_df, label=1, num=500, min_blocks = min_blocks, mix_ratio_range=(0.2, 0.4), mode='kss', shuffle_mode=1)\n",
    "\n",
    "train_mix_3 = pd.concat([train_df, augmented_df_3_1, augmented_df_3_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53203358",
   "metadata": {},
   "source": [
    "#### 3. KoEDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "139dc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = ag_koeda(data=train_df, label=1, num=1000, p=(0.2, 0.1, 0.3, 0.1))\n",
    "train_koeda = pd.concat([train_df, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e52d8",
   "metadata": {},
   "source": [
    "#### 4. Fill masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a12240",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_model_name = \"monologg/kobigbird-bert-base\"  # 허깅페이스에서 모델만 갈아끼우면 됨\n",
    "fill_model = AutoModelForMaskedLM.from_pretrained(fill_model_name)\n",
    "fill_tokenizer = AutoTokenizer.from_pretrained(fill_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f31ac30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]Attention type 'block_sparse' is not possible if sequence_length: 95 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3.Changing attention type to 'original_full'...\n",
      "100%|██████████| 1000/1000 [10:50<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "augmented_df = fill_masked_augmentation(train_df, label_num = 1, random_number = 1000, num_masks_ratio = 0.15)\n",
    "train_fill = pd.concat([train_df, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e19b442",
   "metadata": {},
   "source": [
    "## 실험(원하는 증강법 데이터를 적용시켜서 학습)\n",
    "#### ex) Back translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db4b054e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msungwo101\u001b[0m (\u001b[33mfn_guide_project\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/project/FnGuide/code/wandb/run-20230806_085742-sn6f6n0u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fn_guide_project/fn_guide_project/runs/sn6f6n0u' target=\"_blank\">AD_NEWS_klue/roberta-base</a></strong> to <a href='https://wandb.ai/fn_guide_project/fn_guide_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fn_guide_project/fn_guide_project' target=\"_blank\">https://wandb.ai/fn_guide_project/fn_guide_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fn_guide_project/fn_guide_project/runs/sn6f6n0u' target=\"_blank\">https://wandb.ai/fn_guide_project/fn_guide_project/runs/sn6f6n0u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wandb 기록용\n",
    "hyperparameters={\n",
    "    \"model\": \"klue/roberta-base\",\n",
    "    \"data_ag_method\" : \"back_trans\",\n",
    "    \"Stopword\" : \"True\",    \n",
    "    \"MAX_LEN\" : 512,\n",
    "    \"epochs\" : 3,\n",
    "    \"BATCH_SIZE\" : 8,\n",
    "    'NUM_CORES': 0,\n",
    "    'learning_rate': 2e-5,\n",
    "    'eps' : 1e-8,\n",
    "    'loss' : 'cross_entropy'\n",
    "      }\n",
    "\n",
    "#wandb 초기화\n",
    "wandb.init(project='fn_guide_project',name='AD_NEWS_klue/roberta-base', config = hyperparameters)\n",
    "config = wandb.config\n",
    "\n",
    "train_dataset = TrainTestDataset(train_back, train=True)\n",
    "val_dataset = TrainTestDataset(val_df, train=True)\n",
    "test_dataset = TrainTestDataset(test_df, train=False)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=config.BATCH_SIZE,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=config.NUM_CORES)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                batch_size=config.BATCH_SIZE,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=config.NUM_CORES)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                batch_size=config.BATCH_SIZE,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=config.NUM_CORES)\n",
    "\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=config.learning_rate, eps =config.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab0e8044",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [11:18<00:00,  1.18it/s]\n",
      "100%|██████████| 85/85 [00:26<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:\n",
      "Train Loss: 0.01467119010777354 Accuracy: tensor(0.9588, device='cuda:0') F1 Score (Train): 0.9594\n",
      "Train Precision: 0.9541 Train Recall: 0.9648\n",
      "Validation Accuracy: tensor(0.9573, device='cuda:0') F1 Score (Validation): 0.9490\n",
      "Validation Precision: 0.9507 Validation Recall: 0.9474\n",
      "Validation loss decreased (inf --> 1.030100).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "100%|██████████| 802/802 [11:17<00:00,  1.18it/s]\n",
      "100%|██████████| 85/85 [00:26<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:\n",
      "Train Loss: 0.0071782427284279105 Accuracy: tensor(0.9824, device='cuda:0') F1 Score (Train): 0.9825\n",
      "Train Precision: 0.9818 Train Recall: 0.9833\n",
      "Validation Accuracy: tensor(0.9602, device='cuda:0') F1 Score (Validation): 0.9537\n",
      "Validation Precision: 0.9329 Validation Recall: 0.9754\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [11:17<00:00,  1.18it/s]\n",
      "100%|██████████| 85/85 [00:27<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:\n",
      "Train Loss: 0.004262124366435507 Accuracy: tensor(0.9874, device='cuda:0') F1 Score (Train): 0.9875\n",
      "Train Precision: 0.9879 Train Recall: 0.9870\n",
      "Validation Accuracy: tensor(0.9588, device='cuda:0') F1 Score (Validation): 0.9512\n",
      "Validation Precision: 0.9446 Validation Recall: 0.9579\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "output = os.getenv('HOME')+'/aiffel/project/FnGuide/model/' #자신의 경로로 바꾸기\n",
    "model_name1 = model_name.replace('/','_')\n",
    "\n",
    "folder_name = f\"{model_name1}_{config.data_ag_method}_{config.loss}\"\n",
    "model_save_folder = os.path.join(output, folder_name)\n",
    "os.makedirs(model_save_folder, exist_ok=True)  # 폴더가 없으면 생성\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, verbose=True, path=os.path.join(model_save_folder, f'{model_name1}_{config.data_ag_method}_{config.loss}_stopword_classification_model.pt'))\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "wandb.watch(model,log=\"all\",log_freq=20)  #model: 모니터링할 모델 객체/log:기록할 항목을 지정하는 옵션/log_freq:기록 빈도\n",
    "\n",
    "# loss_fn = Balanced_crossentropy_loss(beta=0.3)\n",
    "# loss_fn = Weighted_crossentropy_loss(pos_weight=3, weight=1)\n",
    "\n",
    "for i in range(config.epochs):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true_train = []  # 학습 데이터의 실제 라벨을 저장할 리스트\n",
    "    y_pred_train = []  # 학습 데이터의 예측 라벨을 저장할 리스트\n",
    "\n",
    "    # Training loop\n",
    "    for input_ids_batch, token_type_id_batch, attention_masks_batch, y_batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        y_batch = y_batch.to(device)\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        token_type_id_batch = token_type_id_batch.to(device)\n",
    "        attention_masks_batch = attention_masks_batch.to(device)\n",
    "        y_pred = model(input_ids_batch, token_type_ids=token_type_id_batch, attention_mask=attention_masks_batch)[0].to(device)\n",
    "        \n",
    "#         loss = loss_fn(y_pred, y_batch) #Balanced_crossentropy_loss 사용시\n",
    "#         loss = loss_function(y_pred, y_batch.view(-1, 1)) #hingeloss 사용시\n",
    "#         loss = FocalLoss()(y_pred, y_batch) #focalloss 사용시\n",
    "        loss = F.cross_entropy(y_pred, y_batch)  # cross_entropy 사용시\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        correct += (predicted == y_batch).sum()\n",
    "        total += len(y_batch)\n",
    "        \n",
    "        # 학습 데이터의 실제 라벨과 예측 라벨을 저장하여 나중에 F1 점수 계산에 사용\n",
    "        y_true_train.extend(y_batch.tolist())\n",
    "        y_pred_train.extend(predicted.tolist())\n",
    "\n",
    "    losses.append(total_loss)\n",
    "    accuracies.append(correct.float() / total)\n",
    "    \n",
    "    f1_train = f1_score(y_true_train, y_pred_train, average='binary')\n",
    "    precision_train = precision_score(y_true_train, y_pred_train, average='binary')\n",
    "    recall_train = recall_score(y_true_train, y_pred_train, average='binary')\n",
    "    f1_scores.append(f1_train)\n",
    "\n",
    "    y_true_val = []  # 검증 데이터의 실제 라벨을 저장할 리스트 초기화\n",
    "    y_pred_val = []  # 검증 데이터의 예측 라벨을 저장할 리스트 초기화\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0 #추가\n",
    "\n",
    "        for val_input_ids_batch, val_token_type_id_batch, val_attention_masks_batch, val_y_batch in tqdm(val_dataloader):\n",
    "            val_y_batch = val_y_batch.to(device)\n",
    "            val_input_ids_batch = val_input_ids_batch.to(device)\n",
    "            val_token_type_id_batch = val_token_type_id_batch.to(device)\n",
    "            val_attention_masks_batch = val_attention_masks_batch.to(device)\n",
    "            val_y_pred = model(val_input_ids_batch, token_type_ids=val_token_type_id_batch, attention_mask=val_attention_masks_batch)[0].to(device)\n",
    "\n",
    "            _, val_predicted = torch.max(val_y_pred, 1)\n",
    "            val_correct += (val_predicted == val_y_batch).sum()\n",
    "            val_total += len(val_y_batch)\n",
    "            \n",
    "            # 검증 데이터의 실제 라벨과 예측 라벨을 저장\n",
    "            y_true_val.extend(val_y_batch.tolist())\n",
    "            y_pred_val.extend(val_predicted.tolist())\n",
    "        \n",
    "        # 검증 데이터의 F1 점수 계산\n",
    "        f1_val = f1_score(y_true_val, y_pred_val, average='binary')\n",
    "        precision_val = precision_score(y_true_val, y_pred_val, average='binary')\n",
    "        recall_val = recall_score(y_true_val, y_pred_val, average='binary')\n",
    "\n",
    "        val_accuracy = val_correct.float() / val_total\n",
    "        val_loss = F.cross_entropy(val_y_pred, val_y_batch)  # validation loss 값 할당\n",
    "        \n",
    "        print(f\"Epoch {i+1}/{config.epochs}:\")\n",
    "        print(\"Train Loss:\", total_loss / total, \"Accuracy:\", correct.float() / total, \"F1 Score (Train):\", f\"{f1_train:.4f}\")\n",
    "        print(\"Train Precision:\", f\"{precision_train:.4f}\", \"Train Recall:\", f\"{recall_train:.4f}\")\n",
    "        print(\"Validation Accuracy:\", val_accuracy, \"F1 Score (Validation):\", f\"{f1_val:.4f}\")\n",
    "        print(\"Validation Precision:\", f\"{precision_val:.4f}\", \"Validation Recall:\", f\"{recall_val:.4f}\")\n",
    "    \n",
    "    wandb.log({\n",
    "        \"Train Loss\": total_loss / total,\n",
    "        \"Train Accuracy\": correct.float() / total,\n",
    "        \"Validation Accuracy\": val_accuracy,\n",
    "        \"Train F1 Score\": f1_train,\n",
    "        \"Validation F1 Score\": f1_val\n",
    "    })\n",
    "    model.train()\n",
    "         \n",
    "    # Early Stopping 적용\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping!! Goog luck!\")\n",
    "        break\n",
    "        \n",
    "    # 모델 저장  \n",
    "    model_save_path = os.path.join(model_save_folder, f'{model_name1}_{config.data_ag_method}_{config.loss}_stopword_classification_model_{i+1}.pt')\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    wandb.save(os.path.join(model_save_folder, f'{model_name1}_{config.data_ag_method}_{config.loss}_stopword_classification_model_{i+1}.pt'))  # 모델 wandb에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8dc201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:26<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9631811487481591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   real_news       0.95      0.99      0.97       389\n",
      "          AD       0.98      0.93      0.96       290\n",
      "\n",
      "    accuracy                           0.96       679\n",
      "   macro avg       0.97      0.96      0.96       679\n",
      "weighted avg       0.96      0.96      0.96       679\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>▁▇█</td></tr><tr><td>Train F1 Score</td><td>▁▇█</td></tr><tr><td>Train Loss</td><td>█▃▁</td></tr><tr><td>Validation Accuracy</td><td>▁█▄</td></tr><tr><td>Validation F1 Score</td><td>▁█▄</td></tr><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>0.98737</td></tr><tr><td>Train F1 Score</td><td>0.98747</td></tr><tr><td>Train Loss</td><td>0.00426</td></tr><tr><td>Validation Accuracy</td><td>0.95876</td></tr><tr><td>Validation F1 Score</td><td>0.95122</td></tr><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>test_accuracy</td><td>0.96318</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AD_NEWS_klue/roberta-base</strong> at: <a href='https://wandb.ai/fn_guide_project/fn_guide_project/runs/sn6f6n0u' target=\"_blank\">https://wandb.ai/fn_guide_project/fn_guide_project/runs/sn6f6n0u</a><br/> View job at <a href='https://wandb.ai/fn_guide_project/fn_guide_project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg3MTY1MTk1/version_details/v5' target=\"_blank\">https://wandb.ai/fn_guide_project/fn_guide_project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg3MTY1MTk1/version_details/v5</a><br/>Synced 5 W&B file(s), 1 media file(s), 3 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230806_085742-sn6f6n0u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "pred = []\n",
    "probabilities = [] # 예측 확률값을 저장할 리스트\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, token_type_id_batch, attention_masks_batch in tqdm(test_dataloader):\n",
    "        y_pred = model(input_ids_batch.to(device),token_type_ids = token_type_id_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        pred.extend(predicted.tolist())\n",
    "        probabilities.extend(F.softmax(y_pred, dim=1).tolist())\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(pred, test_df['label'])\n",
    "    print(accuracy_score(pred, test_df['label']))\n",
    "    classification_report_str = classification_report(test_df['label'], pred, target_names=['real_news', 'AD'])\n",
    "    print(classification_report(test_df['label'], pred, target_names=['real_news', 'AD']))\n",
    "\n",
    "    cm = confusion_matrix(test_df['label'], pred)\n",
    "    sns.heatmap(cm, annot = True, cmap='coolwarm', xticklabels=['real_news', 'AD'], yticklabels=['real_news', 'AD'], fmt='d')\n",
    "    plt.show()\n",
    "\n",
    "    # f1-score 추출\n",
    "    report = classification_report(test_df['label'], pred, target_names=['real_news', 'AD'], output_dict=True)\n",
    "\n",
    "    wandb.log({\"test_accuracy\": accuracy})\n",
    "    wandb.log({\"classification_report\": classification_report_str})\n",
    "    wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                y_true=test_df['label'],\n",
    "                                                                preds=pred,\n",
    "                                                                class_names=['negative', 'positive'])})\n",
    "\n",
    "wandb.finish() #훈련과 평가가 다 끝났을때 해주는 것이 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4581c",
   "metadata": {},
   "source": [
    "- 예측 실패 인덱스, softmax값, 맞춘 인덱스 중 softmax 값이 낮은 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4414833",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check_df = test_df.copy() #테스트 데이터프레임 복사\n",
    "pred1 = pd.Series(pred, name=\"pred\") #예측된 레이블 시리즈로 만들고 이름 붙혀주기\n",
    "test_check_df = pd.concat([test_check_df, pred1], axis = 1) #두개를 하나의 데이터프레임으로 합치기\n",
    "\n",
    "diff_indices = test_check_df[test_check_df['label'] != test_check_df['pred']].index #답변이 틀린것들의 index를 뽑을 수 있음\n",
    "\n",
    "print(\"예측 실패 인덱스 : \", diff_indices)\n",
    "#######################################################################################################################################\n",
    "new_df = test_df.copy()\n",
    "new_df['predicted_label'] = pred\n",
    "new_df['probability_negative'] = [prob[0] for prob in probabilities]\n",
    "new_df['probability_positive'] = [prob[1] for prob in probabilities]\n",
    "\n",
    "softmax_check(diff_indices, new_df)\n",
    "\n",
    "negative_lowest_5 = new_df[(new_df['label'] == 0) & (new_df['label'] == new_df['predicted_label'])].nsmallest(5, 'probability_negative').index\n",
    "positive_lowest_5 = new_df[(new_df['label'] == 1) & (new_df['label'] == new_df['predicted_label'])].nsmallest(5, 'probability_positive').index\n",
    "\n",
    "print(\"Label 0일 때 Probability Negative 값이 가장 낮은 5개 인덱스:\", negative_lowest_5)\n",
    "print(\"Label 1일 때 Probability Positive 값이 가장 낮은 5개 인덱스:\", positive_lowest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296d06e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T03:13:32.132321Z",
     "start_time": "2023-08-16T03:13:32.087319Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "예측 실패 인덱스 :  Int64Index([ 40,  71,  73, 158, 161, 188, 192, 244, 246, 275, 291, 303, 324,\n",
    "            343, 372, 403, 456, 473, 495, 500, 554, 556, 569, 618, 643],\n",
    "           dtype='int64')\n",
    "\n",
    "인덱스: 40\n",
    "Real news(%) :  0.9963169097900391\n",
    "AD news(%) :  0.0036830061580985785\n",
    "실제 답변:1, 예측 답변:0\n",
    "\n",
    "\n",
    "인덱스: 71\n",
    "Real news(%) :  0.9993676543235779\n",
    "AD news(%) :  0.0006323348497971892\n",
    "실제 답변:1, 예측 답변:0\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "Label 0일 때 Probability Negative 값이 가장 낮은 5개 인덱스: Int64Index([505, 439, 395, 536, 385], dtype='int64')\n",
    "Label 1일 때 Probability Positive 값이 가장 낮은 5개 인덱스: Int64Index([104, 156, 492, 390, 585], dtype='int64')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea90b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
