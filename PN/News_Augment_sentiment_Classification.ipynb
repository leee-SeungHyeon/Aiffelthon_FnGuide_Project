{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "676dbf64",
   "metadata": {},
   "source": [
    "# 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab02ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.nn import functional as F\n",
    "import re\n",
    "import pickle\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score, precision_score, recall_score\n",
    "import wandb\n",
    "from konlpy.tag import Mecab\n",
    "import collections\n",
    "from hanspell import spell_checker\n",
    "import kss\n",
    "\n",
    "from evaluate import load\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from ktextaug import TextAugmentation\n",
    "import ktextaug\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelForMaskedLM\n",
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['font.family'] = 'NanumGothic'\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(random_seed)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(random_seed) # current gpu seed\n",
    "    torch.cuda.manual_seed_all(random_seed) # All gpu seed\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb811533",
   "metadata": {},
   "source": [
    "# 2. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b7cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    데이터를 불러오는 함수\n",
    "    \n",
    "    Input:\n",
    "        fiie_name (str): 불러올 파일의 이름\n",
    "    \n",
    "    Output:\n",
    "        df (DataFrame): 불러온 데이터를 데이터프레임으로 할당\n",
    "    \n",
    "    \"\"\"\n",
    "    data_path = os.getenv('HOME') + '/aiffel/mini_aiffelton/data/'#경로는 자신의 경로로 바꿔서 하면 됨\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    df = pd.read_csv(file_path, encoding='utf-8', delimiter='\\t')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d0660",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data('ratings_train.csv')\n",
    "val_df = load_data('ratings_val.csv')\n",
    "test_df = load_data('ratings_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aa3d2e",
   "metadata": {},
   "source": [
    "## 2-1. 사전학습 모델 및 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06116ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"klue/roberta-base\" #허깅페이스에서 모델만 갈아끼우면 됨\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 2) #예측하고자 하는 클래스의 수를 넣어줘야함\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d0d76",
   "metadata": {},
   "source": [
    "# 3. 데이터 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa467252",
   "metadata": {},
   "source": [
    "## 3-1. 데이터 null값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb677af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6af340",
   "metadata": {},
   "source": [
    "## 3-2. 타겟 데이터 불균형 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e74338",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2097fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_df['label'].value_counts()) #데이터프레임의 \"label\" 열에 있는 고유한 레이블의 개수를 계산하여 변수 num_classes에 저장\n",
    "\n",
    "colors = plt.cm.Dark2(np.linspace(0, 1, num_classes)) #matplotlib의 cm 모듈을 사용하여 num_classes 개수에 맞게 컬러 맵을 생성\n",
    "iter_color = iter(colors) #colors 배열에 대한 이터레이터를 생성 / 나중에 반복문에서 각 주제별로 다른 색상을 할당하기 위해 사용\n",
    "\n",
    "train_df['label'].value_counts().plot.barh(title=\"레이블 데이터 불균형 체크\",  # \"labels\" 열의 값들을 세어 막대 그래프로 그림\n",
    "                                                 ylabel=\"Topics\", #plot.barh함수를 사용하여 수평 막대 그래프를 생성\n",
    "                                                 color=colors, #그래프의 제목, 축 레이블, 색상, 크기 등의 설정이 이곳에서 이루어짐\n",
    "                                                 figsize=(9,9))\n",
    "\n",
    "for i, v in enumerate(train_df['label'].value_counts()): #주제별 리뷰 수를 순회\n",
    "    c = next(iter_color) #이터레이터 iter_color에서 다음 색상을 가져와 변수 c에 할당\n",
    "    plt.text(v, i, #막대 그래프에 주제별 리뷰 수와 해당 비율을 텍스트로 표시\n",
    "           \" \"+str(v)+\", \"+str(round(v*100/train_df.shape[0],2))+\"%\",  #v는 리뷰 수, i는 인덱스를 나타냄\n",
    "           color=c, \n",
    "           va='center', \n",
    "           fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80334c8",
   "metadata": {},
   "source": [
    "## 3-3. 문자열의 길이 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768bfe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('문장의 최소 길이 :{}'.format(min(len(l) for l in train_df['document'])))\n",
    "print('문장의 최대 길이 :{}'.format(max(len(l) for l in train_df['document'])))\n",
    "print('문장의 평균 길이 :{}'.format(sum(map(len, train_df['document']))/len(train_df['document'])))\n",
    "\n",
    "plt.hist([len(s) for s in train_df['document']], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab633378",
   "metadata": {},
   "source": [
    "### 3-3-1. 긍정 / 부정 뉴스 문자열 길이 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3337f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_label1 = train_df[train_df['label'] == 1]\n",
    "print('긍정 뉴스 document 최소 길이 :{}'.format(min(len(l) for l in train_df_label1['document'])))\n",
    "print('긍정 뉴스 document 최대 길이 :{}'.format(max(len(l) for l in train_df_label1['document'])))\n",
    "print('긍정 뉴스 document 평균 길이 :{}'.format(sum(map(len, train_df_label1['document']))/len(train_df_label1['document'])))\n",
    "print('-----'*10)\n",
    "train_df_label0 = train_df[train_df['label'] == -1]\n",
    "print('부정 뉴스 document 최소 길이 :{}'.format(min(len(l) for l in train_df_label0['document'])))\n",
    "print('부정 뉴스 document 최대 길이 :{}'.format(max(len(l) for l in train_df_label0['document'])))\n",
    "print('부정 뉴스 document 평균 길이 :{}'.format(sum(map(len, train_df_label0['document']))/len(train_df_label0['document'])))\n",
    "\n",
    "plt.hist([len(s) for s in train_df_label1['document']], bins=50, alpha=0.5, label='긍정 뉴스')\n",
    "plt.hist([len(s) for s in train_df_label0['document']], bins=50, alpha=0.5, label='부정 뉴스')\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56bbb31",
   "metadata": {},
   "source": [
    "## 3-4. 토큰화된 토큰 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a415a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('토큰의 최소 개수: {}'.format(min(len(tokenizer(train_df['document'][l])['input_ids']) for l in range(len(train_df['document'])))))\n",
    "print('토큰의 최대 개수: {}'.format(max(len(tokenizer(train_df['document'][l])['input_ids']) for l in range(len(train_df['document'])))))\n",
    "print('토큰의 평균 개수: {}'.format(sum(len(tokenizer(train_df['document'][l])['input_ids']) for l in range(len(train_df['document'])))/len(train_df['document'])))\n",
    "\n",
    "plt.hist([len(tokenizer(train_df['document'][l])['input_ids']) for l in range(len(train_df['document']))], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3275d",
   "metadata": {},
   "source": [
    "### 3-4-1. 긍정 / 부정 뉴스 토큰화된 토큰 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58cb662",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_label1 = train_df[train_df['label'] == 1]\n",
    "train_df_label1 = pd.DataFrame(train_df_label1['document'],columns=['document']).reset_index(drop=True)\n",
    "print('긍정 뉴스 토큰의 최소 개수: {}'.format(min(len(tokenizer(train_df_label1['document'][l])['input_ids']) for l in range(len(train_df_label1['document'])))))\n",
    "print('긍정 뉴스 토큰의 최대 개수: {}'.format(max(len(tokenizer(train_df_label1['document'][l])['input_ids']) for l in range(len(train_df_label1['document'])))))\n",
    "print('긍정 뉴스 토큰의 평균 개수: {}'.format(sum(len(tokenizer(train_df_label1['document'][l])['input_ids']) for l in range(len(train_df_label1['document'])))/len(train_df_label1['document'])))\n",
    "print('-----'*10)\n",
    "\n",
    "train_df_label0 = train_df[train_df['label'] == -1]\n",
    "train_df_label0 = pd.DataFrame(train_df_label0['document'],columns=['document']).reset_index(drop=True)\n",
    "print('부정 뉴스 토큰의 최소 개수: {}'.format(min(len(tokenizer(train_df_label0['document'][l])['input_ids']) for l in range(len(train_df_label0['document'])))))\n",
    "print('부정 뉴스 토큰의 최대 개수: {}'.format(max(len(tokenizer(train_df_label0['document'][l])['input_ids']) for l in range(len(train_df_label0['document'])))))\n",
    "print('부정 뉴스 토큰의 평균 개수: {}'.format(sum(len(tokenizer(train_df_label0['document'][l])['input_ids']) for l in range(len(train_df_label0['document'])))/len(train_df_label0['document'])))\n",
    "\n",
    "plt.hist([len(tokenizer(train_df_label1['document'][l])['input_ids']) for l in range(len(train_df_label1['document']))], bins=50,alpha=0.5, label='긍정 뉴스')\n",
    "plt.hist([len(tokenizer(train_df_label0['document'][l])['input_ids']) for l in range(len(train_df_label0['document']))], bins=50,alpha=0.5, label='부정 뉴스')\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed78a9",
   "metadata": {},
   "source": [
    "## 3-5. 가장 많은 명사 추출(2글자 이상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b30a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "\n",
    "document = train_df['document'].to_list()\n",
    "n_corpus = []\n",
    "for doc in document:\n",
    "    for n in mecab.nouns(doc): #mecab에서 명사 추출\n",
    "        if len(n) > 1:\n",
    "            n_corpus.append(n)\n",
    "\n",
    "count = collections.Counter(n_corpus)\n",
    "most = count.most_common() #빈도 수 순으로 추출\n",
    "\n",
    "x, y= [], []\n",
    "for word,count in most[:20]:\n",
    "    x.append(word)\n",
    "    y.append(count)\n",
    "\n",
    "plt.figure(figsize=(40,20))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5c06b",
   "metadata": {},
   "source": [
    "### 3-5-1. 긍정 뉴스에 대한 명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0cdaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_label1 = train_df[train_df['label'] == 1]\n",
    "document = train_df_label1['document'].to_list()\n",
    "\n",
    "n_corpus = []\n",
    "for doc in document:\n",
    "    for n in mecab.nouns(doc): #mecab에서 명사 추출\n",
    "        if len(n) > 1:\n",
    "            n_corpus.append(n)\n",
    "\n",
    "count = collections.Counter(n_corpus)\n",
    "most = count.most_common() #빈도 수 순으로 추출\n",
    "\n",
    "x, y= [], []\n",
    "for word,count in most[:20]:\n",
    "    x.append(word)\n",
    "    y.append(count)\n",
    "\n",
    "plt.figure(figsize=(40,20))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272859cd",
   "metadata": {},
   "source": [
    "### 3-5-2. 부정 뉴스에 대한 명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_label0 = train_df[train_df['label'] == -1]\n",
    "document = train_df_label0['document'].to_list()\n",
    "\n",
    "n_corpus = []\n",
    "for doc in document:\n",
    "    for n in mecab.nouns(doc): #mecab에서 명사 추출\n",
    "        if len(n) > 1:\n",
    "            n_corpus.append(n)\n",
    "\n",
    "count = collections.Counter(n_corpus)\n",
    "most = count.most_common() #빈도 수 순으로 추출\n",
    "\n",
    "x, y= [], []\n",
    "for word,count in most[:20]:\n",
    "    x.append(word)\n",
    "    y.append(count)\n",
    "\n",
    "plt.figure(figsize=(40,20))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff1843",
   "metadata": {},
   "source": [
    "# 4. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60eb470",
   "metadata": {},
   "source": [
    "## 4-1. 중복 제거, 컬럼 삭제 및 레이블 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(sentence: str, stopwords: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    입력받은 sentence를 전처리 해주는 함수\n",
    "    \n",
    "    Input:\n",
    "        sentence (str): 전처리 시킬 문장\n",
    "        stopwords (bool): 불용어 사전 사용 여부\n",
    "    Output:\n",
    "        sentence (str): 전처리된 문장\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'\\([^)]*\\)', r'', sentence) #괄호로 둘러싸인 부분 제거\n",
    "    sentence = re.sub(r'#\\w+', '', sentence) #해쉬태그 문자 제거\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)  #문장 내의 구두점을 공백과 함께 분리(숫자 없앰 -> 금융이라 중요하다 생각)\n",
    "    sentence = re.sub(r'[^a-zA-Z가-힣0-9?.!,% ]+', r' ', sentence) #영문 알파벳, 한글, 숫자, 구두점을 제외한 모든 문자를 제거\n",
    "    sentence = re.sub(r\"['\\n']+\", r\"\", sentence) #개행 문자 제거\n",
    "    sentence = re.sub(r'[\"   \"]+', \" \", sentence) #연속된 공백을 하나의 공백으로 변환\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    if stopwords:\n",
    "        words = sentence.split()\n",
    "        filtered_words = [word for word in words if word not in stopwords]\n",
    "        sentence = ' '.join(filtered_words)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: DataFrame, data_type: str, stopwords: bool = False) -> DataFrame:\n",
    "    \"\"\"\n",
    "    데이터프레임의 불필요한 컬럼을 없애주고, document의 한국어 맞춤법 교정을 해주는 함수\n",
    "    \n",
    "    Input:\n",
    "        df (DataFrame): 전처리할 데이터 프레임\n",
    "        data_type (str): 데이터 타입\n",
    "        stopwords (bool):  불용어 사전 사용 여부 \n",
    "        \n",
    "    Output:\n",
    "        df (DataFrame): 전처리된 데이터 프레임\n",
    "    \"\"\"\n",
    "    df = df[['document', 'label']]\n",
    "    df['label'] = df['label'].replace(-1, 0) #감성분석 데이터는 -1이 negative라서 해당 데이터를 0으로 바꿔줬지만, 만약 0,1로 레이블이 되어\n",
    "                                             #있다면 없애도 됨\n",
    "    if data_type=='train':\n",
    "        df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    #################표준어 체크\n",
    "    \n",
    "    new_sentences = []\n",
    "\n",
    "    for sentence in tqdm(df['document']):\n",
    "        \n",
    "        sentence = transform(sentence, stopwords = False)\n",
    "        try:\n",
    "            sentence = sentence.replace('\\\\', '')\n",
    "            spelled_sent = spell_checker.check(sentence)\n",
    "            new_sentence = spelled_sent.checked\n",
    "            if not new_sentence:\n",
    "\n",
    "                new_sentence = sentence\n",
    "            new_sentences.append(new_sentence)\n",
    "        except:\n",
    "\n",
    "            new_sentences.append(sentence)\n",
    "    \n",
    "    df['document'] = new_sentences\n",
    "    \n",
    "    if data_type=='train':\n",
    "        df = df.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7027b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocess_data(train_df, data_type = 'train', stopwords = None)\n",
    "val_df  = preprocess_data(val_df, data_type = 'val', stopwords = None)\n",
    "test_df  = preprocess_data(test_df, data_type = 'test', stopwords = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1752f3",
   "metadata": {},
   "source": [
    "## 4-2. 불용어 사전 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3000e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [] \n",
    "stop_path = os.getenv('HOME') + '/aiffel/mini_aiffelton/data/stopwords.txt'  #경로\n",
    "\n",
    "with open(stop_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.read()\n",
    "\n",
    "    sentences = re.findall(r'\"([^\"]*)\"', lines)  # 작은따옴표로 둘러싸인 내용 추출\n",
    "    for sentence in sentences:\n",
    "        stopwords.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca0ab0",
   "metadata": {},
   "source": [
    "# 5. 데이터 증강"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfded42",
   "metadata": {},
   "source": [
    "## 5-1. KOR_EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f186eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_path = os.getenv('HOME') + '/aiffel/mini_aiffelton/data/wordnet.pickle'\n",
    "wordnet = {}\n",
    "with open(wordnet_path, \"rb\") as f:\n",
    "    wordnet = pickle.load(f)\n",
    "\n",
    "########################################################################\n",
    "# Synonym replacement\n",
    "# Replace n words in the sentence with synonyms from wordnet\n",
    "########################################################################\n",
    "def synonym_replacement(words: list , n: int) -> list:\n",
    "    \"\"\"\n",
    "    stop words가 아닌 특정 단어를 유의어로 교체하여 증강시키는 함수\n",
    "    \n",
    "    Input:\n",
    "        words (list): 증강시킬 단어 리스트\n",
    "        n (int): 대체할 단어 수\n",
    "    Output:\n",
    "        new_words (list): 랜덤으로 선택해서 유의어로 교체된 단어 리스트\n",
    "    \"\"\"\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words])) #입력된 단어 리스트에서 중복을 제거한 후 \n",
    "    random.shuffle(random_word_list)#랜덤하게 순서를 섞어 random_word_list생성\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word) \n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "    if len(new_words) != 0:\n",
    "        sentence = ' '.join(new_words)\n",
    "        new_words = sentence.split(\" \")\n",
    "\n",
    "    else:\n",
    "        new_words = \"\"\n",
    "\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def get_synonyms(word: str) -> list:\n",
    "    \"\"\"\n",
    "    입력된 단어의 동의어를 찾아 리스트로 반환하는 함수\n",
    "    \n",
    "    Input:\n",
    "        word (str): 전처리할 데이터 프레임\n",
    "    Output:\n",
    "        aug_data (dataframe): 전처리된 데이터 프레임\n",
    "    \"\"\"\n",
    "    synomyms = []\n",
    "\n",
    "    try:\n",
    "        for syn in wordnet[word]: # WordNet을 통해 동의어를 가져옴\n",
    "            for s in syn:\n",
    "                synomyms.append(syn) #가져온 동의어 세트를 순회하면서 각 동의어를 synonyms 리스트에 추가\n",
    "    except:\n",
    "        pass #예외가 발생하면 (wordnet[word]에서 단어에 대한 동의어가 없을 경우) pass를 실행\n",
    "\n",
    "    return synomyms\n",
    "\n",
    "########################################################################\n",
    "# Random deletion\n",
    "# Randomly delete words from the sentence with probability p\n",
    "########################################################################\n",
    "def random_deletion(words: list, p: float) -> list:\n",
    "    \"\"\"\n",
    "    문장 내에서 일정 확률로 임의의 단어를 선택해 제거하여 증강시키는 함수\n",
    "    \n",
    "    Input:\n",
    "        words (list): 증강시킬 단어 리스트\n",
    "        n (int): 임의의 단어를 제거시킬 확률\n",
    "    Output:\n",
    "        new_words (list): 임의의 단어를 제거한 단어 리스트\n",
    "    \"\"\"\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "\n",
    "    new_words = []\n",
    "    for word in words: #입력된 단어 리스트를 순회하면서 임의의 단어를 선택하여 제거하는 과정\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "\n",
    "    if len(new_words) == 0: #모든 단어가 제거된 경우에는 원래 단어 리스트에서 임의로 하나의 단어를 선택하여 반환\n",
    "        rand_int = random.randint(0, len(words)-1)\n",
    "        return [words[rand_int]]\n",
    "\n",
    "    return new_words\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "########################################################################\n",
    "def random_swap(words: list, n: int) -> list:\n",
    "    \"\"\"\n",
    "    문장 내에서 일정 횟수만큼 랜덤한 단어들을 서로 교체하여 증강시키는 함수\n",
    "    \n",
    "    Input:\n",
    "        words (list): 증강시킬 단어 리스트\n",
    "        n (int): 교체할 단어 쌍의 수\n",
    "    Output:\n",
    "        new_words (list): 랜덤하게 단어를 교체한 단어 리스트\n",
    "    \"\"\"\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "\n",
    "    return new_words\n",
    "\n",
    "def swap_word(new_words: list) -> list:\n",
    "    \"\"\"\n",
    "    단어 리스트 내에서 랜덤한 두 개의 단어를 서로 교체 함수\n",
    "    \n",
    "    Input:\n",
    "        new_words (list): 단어 리스트\n",
    "    Output:\n",
    "        new_words (list): 단어를 교체한 단어 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    counter = 0\n",
    "\n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "        counter += 1\n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "\n",
    "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "    return new_words\n",
    "\n",
    "########################################################################\n",
    "# Random insertion\n",
    "# Randomly insert n words into the sentence\n",
    "########################################################################\n",
    "def random_insertion(words: list, n: int) -> list:\n",
    "    \"\"\"\n",
    "    문장 내에서 일정 횟수만큼 랜덤한 단어들을 삽입하여 증강시키는 함수\n",
    "    \n",
    "    Input:\n",
    "        words (list): 증강시킬 단어 리스트\n",
    "        n (int): 삽입할 단어의 수\n",
    "    Output:\n",
    "        new_words (list): 랜덤하게 단어를 삽입한 단어 리스트\n",
    "    \"\"\"\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "    \n",
    "    return new_words\n",
    "\n",
    "\n",
    "def add_word(new_words: list) -> None:\n",
    "    \"\"\"\n",
    "    단어 리스트 내에 랜덤한 위치에 랜덤한 동의어를 삽입하는 함수\n",
    "    \n",
    "    Input:\n",
    "        new_words (list): 단어 리스트\n",
    "    Output:\n",
    "        None\n",
    "    \"\"\"\n",
    "  \n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    while len(synonyms) < 1:\n",
    "        if len(new_words) >= 1:\n",
    "            random_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "            synonyms = get_synonyms(random_word)\n",
    "            counter += 1\n",
    "        else:\n",
    "            random_word = \"\"\n",
    "\n",
    "        if counter >= 10:\n",
    "            return\n",
    "        \n",
    "    random_synonym = synonyms[0]\n",
    "    random_idx = random.randint(0, len(new_words)-1)\n",
    "    new_words.insert(random_idx, random_synonym)\n",
    "\n",
    "\n",
    "def EDA(sentence: str, alpha_sr: float = 0.1, alpha_ri: float = 0.1, alpha_rs: float = 0.1, p_rd: float = 0.1) -> list:\n",
    "    \"\"\"\n",
    "    문장을 다양한 증강 기법을 적용하여 증강된 문장 리스트를 반환하는 함수\n",
    "    \n",
    "    Input:\n",
    "        sentence (str): 증강할 문장\n",
    "        alpha_sr (float): synonym replacement의 증강 비율\n",
    "        alpha_ri (float): random insertion의 증강 비율\n",
    "        alpha_rs (float): random swap의 증강 비율\n",
    "        p_rd (float): random deletion의 제거 비율\n",
    "        num_aug (int): 생성할 증강 문장 수\n",
    "    Output:\n",
    "        augmented_sentences (list): 증강된 문장 리스트\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = transform(sentence)\n",
    "    words = sentence.split(' ')\n",
    "    words = [word for word in words if word is not \"\"]\n",
    "    num_words = len(words)\n",
    "\n",
    "    augmented_sentences = []\n",
    "\n",
    "    n_sr = max(1, int(alpha_sr*num_words))\n",
    "    n_ri = max(1, int(alpha_ri*num_words))\n",
    "    n_rs = max(1, int(alpha_rs*num_words))\n",
    "    \n",
    "    random_number = random.randint(1, 4)\n",
    "    # SR\n",
    "    if random_number == 1:    \n",
    "        a_words = synonym_replacement(words, n_sr)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    # RI\n",
    "    elif random_number == 2:\n",
    "        a_words = random_insertion(words, n_ri)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "    \n",
    "    # RS\n",
    "    elif random_number == 3:\n",
    "        a_words = random_swap(words, n_rs)\n",
    "        augmented_sentences.append(\" \".join(a_words))\n",
    "\n",
    "    # RD\n",
    "    else:\n",
    "        a_words = random_deletion(words, p_rd)\n",
    "        augmented_sentences.append(\" \".join(a_words))\n",
    "\n",
    "    return augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kor_eda(df: DataFrame, label_num: int, random_number: int,alpha_sr: float = 0.1, alpha_ri: float = 0.1, alpha_rs: float = 0.1, p_rd: float = 0.1) -> DataFrame:\n",
    "    \"\"\"\n",
    "    주어진 데이터프레임에서 특정 레이블을 가진 문장을 선택하여 EDA를 적용한 후, 증강된 데이터프레임을 반환하는 함수\n",
    "    \n",
    "    Input:\n",
    "        df (DataFrame): 증강을 수행할 데이터프레임\n",
    "        label_num (int): 증강을 수행할 레이블 번호\n",
    "        random_number (int): 증강시킬 문장의 수\n",
    "        alpha_sr (float): synonym replacement의 증강 비율\n",
    "        alpha_ri (float): random insertion의 증강 비율\n",
    "        alpha_rs (float): random swap의 증강 비율\n",
    "        p_rd (float): random deletion의 제거 비율\n",
    "        \n",
    "    Output:\n",
    "        aug_train_data (DataFrame): 증강된 데이터프레임\n",
    "    \"\"\"\n",
    "    label_data = df[df['label'] == label_num]\n",
    "    \n",
    "    random_sentences = random.sample(label_data['document'].tolist(), random_number)\n",
    "    \n",
    "    new_sentences = []\n",
    "    \n",
    "    for i in tqdm(random_sentences):\n",
    "        new_sentences.extend(EDA(i, alpha_sr=alpha_sr, alpha_ri=alpha_ri, alpha_rs=alpha_rs, p_rd=p_rd))\n",
    "            \n",
    "    result = pd.DataFrame({'document': new_sentences, 'label': label_num})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train_data = kor_eda(train_df, 0, 1200, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data = pd.concat([train_df, aug_train_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84f4c5",
   "metadata": {},
   "source": [
    "## 5-2. Fill_Masked_Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958785bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_model_name = 'monologg/kobigbird-bert-base'\n",
    "\n",
    "fill_model = AutoModelForMaskedLM.from_pretrained(fill_model_name)\n",
    "fill_tokenizer = AutoTokenizer.from_pretrained(fill_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb519fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_masked_augmentation(df: DataFrame, label_num: int, random_number: int, num_masks_ratio: float) -> DataFrame:\n",
    "    \"\"\"\n",
    "    마스킹 기법을 통하여 데이터를 증강시키는 함수\n",
    "    \n",
    "    Input:\n",
    "        df (DataFrame): 증강시킬 데이터가 있는 데이터 프레임\n",
    "        label_num (int): 증강시키고 싶은 label number\n",
    "        random_number (int): 증강시킬 데이터의 수\n",
    "        num_masks_ratio (float): 본문에서 마스킹 시킬 비율\n",
    "        \n",
    "    Output:\n",
    "        aug_data (DataFrame): 증강시킨 데이터가 있는 데이터 프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    label_data = df[df['label'] == label_num]\n",
    "    predicted_tokens_list = []\n",
    "    fill_model.to(device)\n",
    "    random.seed(random_seed) #이렇게 하니까 증강문이 고정됨(마스킹도 고정)\n",
    "    \n",
    "    for _ in tqdm(range(random_number)):\n",
    "        \n",
    "        random_sentences = random.choice(label_data['document'].tolist()) \n",
    "\n",
    "        num_masks = int(len(random_sentences.split()) * num_masks_ratio)\n",
    "        num_masks = min(num_masks, 20)\n",
    "        words = random_sentences.split()\n",
    "        masked_indices = random.sample(range(len(words)), num_masks)\n",
    "        masked_text = ' '.join('[MASK]' if i in masked_indices else word for i, word in enumerate(words))\n",
    "        tokenized_sentence = fill_tokenizer.tokenize(masked_text)\n",
    "        masked_indices = [i for i, token in enumerate(tokenized_sentence) if token == fill_tokenizer.mask_token]\n",
    "        \n",
    "        predicted_tokens = []\n",
    "        for index in masked_indices:\n",
    "            tokens = tokenized_sentence.copy()\n",
    "            tokens[index] = fill_tokenizer.mask_token\n",
    "            input_ids = fill_tokenizer.convert_tokens_to_ids(tokens)\n",
    "            inputs = torch.tensor([input_ids]).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = fill_model(inputs)\n",
    "                predictions = outputs.logits[0, index].topk(k=5)\n",
    "            \n",
    "            max_count = 5\n",
    "            count = 0\n",
    "            random_prediction_index = torch.multinomial(predictions.values, 1)  # 상위 5개 중에서 랜덤하게 하나 선택\n",
    "            random_prediction_token_id = predictions.indices[random_prediction_index].item()  # 선택한 토큰 ID 추출\n",
    "            random_prediction_token = fill_tokenizer.convert_ids_to_tokens([random_prediction_token_id])[0]  # 토큰으로 변환\n",
    "            \n",
    "            while random_prediction_token in [\"(\",\")\",\" \",\".\", \",\", \"'\", '\"',\"‘\",\"’\",'“','”',\"[SEP]\", \"[PAD]\", \"[UNK]\"]:\n",
    "                count += 1\n",
    "                if count >= max_count:\n",
    "                    random_prediction_token = \" \"\n",
    "                    break\n",
    "                random_prediction_index = torch.multinomial(predictions.values, 1)  # 상위 5개 중에서 랜덤하게 하나 선택\n",
    "                random_prediction_token_id = predictions.indices[random_prediction_index].item()  # 선택한 토큰 ID 추출\n",
    "                random_prediction_token = fill_tokenizer.convert_ids_to_tokens([random_prediction_token_id])[0]  # 토큰으로 변환\n",
    "                \n",
    "\n",
    "            predicted_tokens.append(random_prediction_token)\n",
    "\n",
    "        new_sentence = tokenized_sentence.copy()\n",
    "\n",
    "        for index, predicted_token in zip(masked_indices, predicted_tokens):\n",
    "            new_sentence[index] = predicted_token\n",
    "\n",
    "        new_sentence = fill_tokenizer.convert_tokens_to_string(new_sentence).strip()\n",
    "        predicted_tokens_list.append(new_sentence)\n",
    "\n",
    "    aug_data = pd.DataFrame({'document': predicted_tokens_list, 'label': label_num})\n",
    "\n",
    "    return aug_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07113d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train_data = fill_masked_augmentation(train_df, label_num = 0, random_number = 1200 , num_masks_ratio = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50028878",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data = pd.concat([train_df, aug_train_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c378e48",
   "metadata": {},
   "source": [
    "## 5-3. Mix_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text: list, mode: str ='kss') -> list:\n",
    "    \"\"\"\n",
    "    문장을 분리시키는 함수\n",
    "    \n",
    "    Input:\n",
    "        text (list): 문장을 분리시킬 list\n",
    "        mode (str): 문장을 분리시킬 모드\n",
    "        \n",
    "    Output:\n",
    "        sentences (list): 문장을 분리시킨 list\n",
    "    \"\"\"\n",
    "    if mode == 'to':\n",
    "        sentences = tokenizer.tokenize(text)  # 토크나이저를 사용하여 문장 단위로 분리\n",
    "    else:\n",
    "        sentences = kss.split_sentences(text)  # Split sentences using kss library\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def sentence_shuffle(text: list, min_blocks: int) -> str:\n",
    "    \"\"\"\n",
    "    한 문단에서 문장을 뒤섞는 함수\n",
    "    \n",
    "    Input:\n",
    "        text (list): 문장을 뒤섞을 문단\n",
    "        min_blocks (int): 최소 문장 수\n",
    "        \n",
    "    Output:\n",
    "        augmented_text (str): 문장을 뒤섞을 문단\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = tokenize_sentences(text, 'kss') \n",
    "    \n",
    "    # 분리된 문장들 중에서 하나 이상의 문장 블록을 선택하기 위해 최소 블록 개수를 설정\n",
    "    min_blocks = min(min_blocks, len(sentences))\n",
    "    \n",
    "    # 무작위로 두 개 이상의 문장 블록을 선택\n",
    "    num_blocks_to_shuffle = random.randint(min_blocks, len(sentences)) #min_blocks부터 len(sentences) 사이의 정수 에서 무작위로 하나의 정수를 반환하는 함수\n",
    "    selected_blocks_indices = random.sample(range(len(sentences)), num_blocks_to_shuffle)\n",
    "    \n",
    "    # 선택된 문장 블록들의 위치를 서로 바꿈\n",
    "    shuffled_sentences = [sentences[i] for i in range(len(sentences)) if i not in selected_blocks_indices]\n",
    "    for i in selected_blocks_indices:\n",
    "        shuffled_sentences.append(sentences[i])\n",
    "    \n",
    "    # 문장 블록들을 다시 하나의 문자열로 합쳐서 증강된 문장을 생성\n",
    "    augmented_text = ' '.join(shuffled_sentences)\n",
    "    \n",
    "    return augmented_text.strip()\n",
    "\n",
    "def mix_document(data: DataFrame, label: int, num: int, min_blocks: int, mix_ratio_range: float = (0.2, 0.4), mode: str = 'kss') -> DataFrame:\n",
    "    \"\"\"\n",
    "    주어진 데이터로부터 문서를 생성하거나 문장을 섞어 증강 데이터를 생성하는 함수\n",
    "    \n",
    "    Input:\n",
    "        data (DataFrame): 원본 데이터를 담고 있는 DataFrame\n",
    "        label: (int): 증강 데이터에 할당할 레이블\n",
    "        num (int): 생성할 증강 데이터의 개수\n",
    "        min_blocks (int): 최소 문장 수\n",
    "        mix_ratio_range (tuple, optional): 문장 제거 및 추가할 비율 범위 (default: (0.2, 0.4))\n",
    "        mode (str, optional): 문장 토큰화 방식 ('kss' 또는 'to') (default: 'kss')\n",
    "        \n",
    "    Output:\n",
    "        augmented_df (DataFrame): 생성된 증강 데이터를 담고 있는 DataFrame\n",
    "    \"\"\"\n",
    "    augmented_data = []\n",
    "    filtered_data = data[data['label'] == label]['document'].tolist()\n",
    "    \n",
    "    selected_data_set = set()  # 중복 선택된 쌍을 추적하기 위한 집합\n",
    "    \n",
    "    for _ in range(num):\n",
    "        random_number = random.randint(0, 1)\n",
    "        \n",
    "        if random_number == 0:\n",
    "        \n",
    "            selected_data = random.sample(filtered_data, 2)  # 원본 데이터 중에서 2개를 무작위로 선택\n",
    "\n",
    "            while tuple(selected_data) in selected_data_set:\n",
    "                selected_data = random.sample(filtered_data, 2)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "            selected_data_set.add(tuple(selected_data))\n",
    "\n",
    "            sentences_1 = tokenize_sentences(selected_data[0], mode)  # 첫 번째 선택된 데이터의 문장으로 분리\n",
    "            sentences_2 = tokenize_sentences(selected_data[1], mode)  # 두 번째 선택된 데이터의 문장으로 분리\n",
    "\n",
    "            mix_ratio_1 = random.uniform(*mix_ratio_range)\n",
    "            mix_ratio_2 = random.uniform(*mix_ratio_range)\n",
    "\n",
    "            # sentences_1에서 특정 비율만큼 문장 제거\n",
    "            num_sentences_to_remove = int(len(sentences_1) * mix_ratio_1)\n",
    "            removed_sentences = random.sample(sentences_1, num_sentences_to_remove)\n",
    "            augmented_sentences_1 = [sentence for sentence in sentences_1 if sentence not in removed_sentences]\n",
    "\n",
    "\n",
    "            # sentences_2에서 특정 비율만큼 문장 추출\n",
    "            num_sentences_to_extract_2 = int(len(sentences_2) * mix_ratio_2)\n",
    "            extracted_sentences_2 = random.sample(sentences_2, num_sentences_to_extract_2)\n",
    "\n",
    "            # 추출된 문장들을 랜덤하게 위치에 삽입하여 augmented_sentences_1에 삽입\n",
    "            for sentence in extracted_sentences_2:\n",
    "                random_index = random.randint(0, len(augmented_sentences_1))\n",
    "                augmented_sentences_1.insert(random_index, sentence)\n",
    "\n",
    "            if mode == 'to':\n",
    "                augmented_text = tokenizer.convert_tokens_to_string(augmented_sentences_1)  # 토큰을 다시 문자열로 변환하여 문서로 만듦\n",
    "            else:\n",
    "                augmented_text = ' '.join(augmented_sentences_1)  # 문장 내의 단어들을 공백으로 연결하여 문장으로 만듦\n",
    "\n",
    "            augmented_data.append(augmented_text)\n",
    "            \n",
    "        elif random_number == 1:\n",
    "        \n",
    "            selected_data = random.choice(filtered_data)# 원본 데이터 중에서 1개를 무작위로 선택\n",
    "\n",
    "            while tuple(selected_data) in selected_data_set:\n",
    "                selected_data = random.choice(filtered_data)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "            selected_data_set.add(tuple(selected_data))\n",
    "\n",
    "            augmented_text = sentence_shuffle(selected_data, min_blocks)\n",
    "            augmented_data.append(augmented_text)\n",
    "        \n",
    "    # 생성된 증강 데이터에 레이블 1 추가\n",
    "    augmented_df = pd.DataFrame({'document': augmented_data, 'label': label})\n",
    "        \n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c0efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train_data = mix_document(train_df, 0, 1200, min_blocks = 3, mix_ratio_range=(0.2, 0.4), mode='kss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c80403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data = pd.concat([train_df, aug_train_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866a01f2",
   "metadata": {},
   "source": [
    "## 5-4. back_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate(data: DataFrame, label, num, mode=\"back_translate\", target_language='en', prob=0.4):\n",
    "    \"\"\"\n",
    "    주어진 데이터를 역번역을 통해 증강하거나 노이즈를 추가하여 증강 데이터를 생성하는 함수\n",
    "    \n",
    "    Input:\n",
    "        data (DataFrame): 원본 데이터를 담고 있는 DataFrame\n",
    "        label: (int): 증강 데이터에 할당할 레이블\n",
    "        num (int): 생성할 증강 데이터의 개수\n",
    "        mode (str): 증강 방식 ('back_translate' 또는 'noise_add') (default: 'back_translate')\n",
    "        target_language (str): 역번역에 사용할 언어 코드 (default: 'en')\n",
    "        prob (float, optional): noise_add 방식일 때 노이즈 추가 확률 (default: 0.4)\n",
    "        \n",
    "    Output:\n",
    "        augmented_df (DataFrame): 생성된 증강 데이터를 담고 있는 DataFrame\n",
    "    \"\"\"\n",
    "    # TextAugmentation 객체 생성\n",
    "    agent = TextAugmentation(tokenizer=\"mecab\", num_processes=1)\n",
    "\n",
    "    # 선택한 레이블로 데이터 필터링\n",
    "    filtered_data = data[data['label'] == label]['document'].tolist()\n",
    "\n",
    "    # 데이터 증강\n",
    "    augmented_data = []\n",
    "    selected_data_set = set()  # 중복 선택된 데이터를 추적하기 위한 집합\n",
    "\n",
    "    for _ in tqdm(range(num)):\n",
    "        selected_data = random.choice(filtered_data)  # 원본 데이터 중에서 무작위로 선택\n",
    "\n",
    "        while selected_data in selected_data_set:\n",
    "            selected_data = random.choice(filtered_data)  # 중복된 데이터가 선택되면 다시 선택\n",
    "\n",
    "        selected_data_set.add(selected_data)  # 선택된 데이터를 집합에 추가\n",
    "\n",
    "        # 데이터 증강\n",
    "        if mode == \"noise_add\":\n",
    "            generated_data = agent.generate(selected_data, mode=\"noise_add\", prob=prob, noise_mode=['phonological_change', 'vowel_change', 'jamo_split'])\n",
    "        else:\n",
    "            generated_data = agent.generate(selected_data, mode=\"back_translate\", target_language=target_language)  #일본어로 하려면 target_language='jp'\n",
    "        \n",
    "        augmented_data.append(generated_data)\n",
    "\n",
    "    # 생성된 데이터를 DataFrame으로 변환\n",
    "    augmented_df = pd.DataFrame({'document': augmented_data, 'label': label})\n",
    "    \n",
    "    return augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train_data = back_translate(data=train_df, label=1, num=970, mode=\"back_translate\", target_language='es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data = pd.concat([train_df, aug_train_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0fa2e0",
   "metadata": {},
   "source": [
    "# 6. 데이터 클래스 및 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9338ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=True, train=True, stopwords=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train: #train data이면 sentence와 label 같이 전처리\n",
    "            labels = self.data['label'][index]\n",
    "            sentence = self.data['document'][index]\n",
    "        else: #test data이면 label만 같이 전처리\n",
    "            sentence = self.data['document'][index]\n",
    "\n",
    "        if self.transform: #앞서 정의한 transform 함수로 sentence 전처리\n",
    "            sentence = transform(sentence, stopwords=self.stopwords)\n",
    "\n",
    "        encoded_dict = tokenizer.encode_plus( # Hugging Face의 토크나이저를 초기화한 후 사용\n",
    "            sentence, #인코딩할 대상 문장\n",
    "            add_special_tokens=True, #특수 토큰(Special Token)을 문장에 추가할지 여부를 지정\n",
    "            max_length=config.MAX_LEN, #문장의 최대 길이를 지정 / 길이가 MAX_LEN보다 긴 문장은 자름\n",
    "            padding='max_length', #문장을 패딩하여 동일한 길이로 맞춰줌 / 패딩은 문장의 뒷부분에 [PAD] 토큰을 추가\n",
    "            truncation=True, #문장이 최대 길이를 초과할 경우 자르기(truncation)를 수행\n",
    "            return_tensors='pt' #결과를 PyTorch의 텐서로 반환\n",
    "        )\n",
    "\n",
    "        padded_token_list = encoded_dict['input_ids'][0]\n",
    "        token_type_id = encoded_dict['token_type_ids'][0]\n",
    "        att_mask = encoded_dict['attention_mask'][0]\n",
    "\n",
    "        if self.train: #train일 경우 padded_token_list, token_type_id, att_mask, target을 반환\n",
    "            target = torch.tensor(labels)\n",
    "            sample = (padded_token_list, token_type_id, att_mask, target)\n",
    "        else: #test일 경우 padded_token_list, token_type_id, att_mask을 반환\n",
    "            sample = (padded_token_list, token_type_id, att_mask)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9fdd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'model': \"kfdeberta-base\",\n",
    "    'data':'-',\n",
    "    'data_ag_method': 'None',\n",
    "    'stopword':'False',\n",
    "    'MAX_LEN': 512,\n",
    "    'loss':'cross_entropy',\n",
    "    'epochs': 3,\n",
    "    'BATCH_SIZE': 8,\n",
    "    'NUM_CORES': 0,\n",
    "    'learning_rate': 1e-5,\n",
    "    'eps' : 1e-8,\n",
    "}\n",
    "\n",
    "#wandb 초기화\n",
    "wandb.init(project='fn_guide_project',name='PN_NEWS_kfdeberta-base', config = hyperparameters)\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34563fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainTestDataset(train_df, transform=True, train = True, stopwords=False)\n",
    "val_dataset = TrainTestDataset(val_df, transform=True, train = True, stopwords=False)\n",
    "test_dataset = TrainTestDataset(test_df, transform=True, train = False, stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ea54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                        batch_size=config.BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                      num_workers=config.NUM_CORES)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                        batch_size=config.BATCH_SIZE,\n",
    "                                        shuffle=False,\n",
    "                                      num_workers=config.NUM_CORES)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                        batch_size=config.BATCH_SIZE,\n",
    "                                        shuffle=False,\n",
    "                                      num_workers=config.NUM_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353725c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=config.learning_rate, eps =config.eps)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72361cec",
   "metadata": {},
   "source": [
    "# 7. 모델 학습 및 loss 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb110988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (self.alpha * (1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e9054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(HingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # 이진 분류를 위해 출력 텐서를 (배치 크기, 2) 모양으로 변경\n",
    "        outputs = outputs.view(-1, 2)\n",
    "\n",
    "        # 모델의 출력 디바이스에 맞추기\n",
    "        targets = targets.to(outputs.device)\n",
    "\n",
    "        # 이중 손실 계산\n",
    "        loss = torch.max(torch.tensor(0.0).to(outputs.device), 1 - outputs * (targets * 2 - 1))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Balanced_crossentropy_loss(nn.Module):\n",
    "    def __init__(self, beta=0.5):\n",
    "        super(Balanced_crossentropy_loss, self).__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: 모델의 출력 logits\n",
    "        # targets: 실제 레이블 (0 또는 1)\n",
    "\n",
    "        # 확률값으로 변환하여 클래스 1에 해당하는 값만 선택\n",
    "        prob = torch.softmax(inputs, dim=1)\n",
    "        prob_class_1 = prob[:, 1]\n",
    "\n",
    "        # 클래스별 샘플 수 계산\n",
    "        class_count = torch.bincount(targets, minlength=2)  # 이진 분류에서는 minlength=2로 설정합니다.\n",
    "\n",
    "        # 클래스 빈도에 반비례하는 가중치 계산\n",
    "        weights = 1.0 / (class_count.float() + 1e-8)\n",
    "\n",
    "        # 클래스별 가중치를 적용하여 손실 계산\n",
    "        weight = weights[targets]\n",
    "\n",
    "        # Balanced Cross Entropy Loss 계산\n",
    "        loss = -weight * prob_class_1\n",
    "\n",
    "        # 모든 샘플에 대한 손실 평균 계산\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc9550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighted_crossentropy_loss(nn.Module):\n",
    "    def __init__(self, pos_weight=None, weight=None, reduction='mean'):\n",
    "        super(Weighted_crossentropy_loss, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets_one_hot = torch.eye(2)[targets].to(inputs.device)  # one-hot 인코딩, inputs와 동일한 디바이스로 이동\n",
    "        weight_tensor = torch.tensor(self.weight).to(inputs.device)  # weight를 Tensor로 변환, inputs와 동일한 디바이스로 이동\n",
    "        pos_weight_tensor = torch.tensor(self.pos_weight).to(inputs.device)  # pos_weight를 Tensor로 변환, inputs와 동일한 디바이스로 이동\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets_one_hot, pos_weight=pos_weight_tensor, weight=weight_tensor, reduction=self.reduction)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=3, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            \n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38406499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "#FocalLoss 모델 이름 저장 다시 한번 확인해야함 -> 기존에 있던 모델에 덮어씌워질 수 있음(주의하기)\n",
    "output = os.getenv('HOME')+'/aiffel/mini_aiffelton/results/' #자신의 경로로 바꾸기\n",
    "early_stopping = EarlyStopping(patience=3, verbose=True, path=os.path.join(output, f'{config.model}_{config.data_ag_method}_{config.loss}_classification_model.pt'))\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "wandb.watch(model,log=\"all\",log_freq=20)  #model: 모니터링할 모델 객체/log:기록할 항목을 지정하는 옵션/log_freq:기록 빈도\n",
    "\n",
    "# loss_fn = Balanced_crossentropy_loss(beta=0.3)\n",
    "# loss_fn = Weighted_crossentropy_loss(pos_weight=3, weight=1)\n",
    "\n",
    "for i in range(config.epochs):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    y_true_train = []  # 학습 데이터의 실제 라벨을 저장할 리스트\n",
    "    y_pred_train = []  # 학습 데이터의 예측 라벨을 저장할 리스트\n",
    "    \n",
    "    # Training loop\n",
    "    for input_ids_batch, token_type_id_batch, attention_masks_batch, y_batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        y_batch = y_batch.to(device)\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        token_type_id_batch = token_type_id_batch.to(device)\n",
    "        attention_masks_batch = attention_masks_batch.to(device)\n",
    "        y_pred = model(input_ids_batch, token_type_ids=token_type_id_batch, attention_mask=attention_masks_batch)[0].to(device)\n",
    "        \n",
    "#         loss = loss_fn(y_pred, y_batch) #Balanced_crossentropy_loss 사용시\n",
    "#         loss = loss_function(y_pred, y_batch.view(-1, 1)) #hingeloss 사용시\n",
    "#         loss = FocalLoss()(y_pred, y_batch) #focalloss 사용시\n",
    "        loss = F.cross_entropy(y_pred, y_batch)  # cross_entropy 사용시\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        correct += (predicted == y_batch).sum()\n",
    "        total += len(y_batch)\n",
    "        \n",
    "        # 학습 데이터의 실제 라벨과 예측 라벨을 저장하여 나중에 F1 점수 계산에 사용\n",
    "        y_true_train.extend(y_batch.tolist())\n",
    "        y_pred_train.extend(predicted.tolist())\n",
    "\n",
    "    losses.append(total_loss)\n",
    "    accuracies.append(correct.float() / total)\n",
    "\n",
    "    f1_train = f1_score(y_true_train, y_pred_train, average='binary')\n",
    "    precision_train = precision_score(y_true_train, y_pred_train, average='binary')\n",
    "    recall_train = recall_score(y_true_train, y_pred_train, average='binary')\n",
    "\n",
    "    y_true_val = []  # 검증 데이터의 실제 라벨을 저장할 리스트 초기화\n",
    "    y_pred_val = []  # 검증 데이터의 예측 라벨을 저장할 리스트 초기화\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0 #추가\n",
    "\n",
    "        for val_input_ids_batch, val_token_type_id_batch, val_attention_masks_batch, val_y_batch in tqdm(val_dataloader):\n",
    "            val_y_batch = val_y_batch.to(device)\n",
    "            val_input_ids_batch = val_input_ids_batch.to(device)\n",
    "            val_token_type_id_batch = val_token_type_id_batch.to(device)\n",
    "            val_attention_masks_batch = val_attention_masks_batch.to(device)\n",
    "            val_y_pred = model(val_input_ids_batch, token_type_ids=val_token_type_id_batch, attention_mask=val_attention_masks_batch)[0].to(device)\n",
    "\n",
    "            _, val_predicted = torch.max(val_y_pred, 1)\n",
    "            val_correct += (val_predicted == val_y_batch).sum()\n",
    "            val_total += len(val_y_batch)\n",
    "            \n",
    "            # 검증 데이터의 실제 라벨과 예측 라벨을 저장\n",
    "            y_true_val.extend(val_y_batch.tolist())\n",
    "            y_pred_val.extend(val_predicted.tolist())\n",
    "        \n",
    "        # 검증 데이터의 F1 점수 계산\n",
    "        f1_val = f1_score(y_true_val, y_pred_val, average='binary')\n",
    "        precision_val = precision_score(y_true_val, y_pred_val, average='binary')\n",
    "        recall_val = recall_score(y_true_val, y_pred_val, average='binary')\n",
    "\n",
    "        val_accuracy = val_correct.float() / val_total\n",
    "        val_loss = F.cross_entropy(val_y_pred, val_y_batch)  # validation loss 값 할당\n",
    "        \n",
    "        print(f\"Epoch {i+1}/{config.epochs}:\")\n",
    "        print(\"Train Loss:\", total_loss / total, \"Accuracy:\", correct.float() / total)\n",
    "        print(\"Train Precision:\", f\"{precision_train:.4f}\", \"Train Recall:\", f\"{recall_train:.4f}\", \"Train F1:\", f\"{f1_train:.4f}\")\n",
    "        print(\"Validation Accuracy:\", val_accuracy)\n",
    "        print(\"Validation Precision:\", f\"{precision_val:.4f}\", \"Validation Recall:\", f\"{recall_val:.4f}\", \"Validation F1:\", f\"{f1_val:.4f}\")\n",
    "    \n",
    "    \n",
    "    wandb.log({\"train_loss\": total_loss / total, \"train_accuracy\": correct.float() / total, \"val_accuracy\": val_accuracy, \"val_loss\": val_loss, \"train_F1\": f1_train,\n",
    "        \"val_F1 Score\": f1_val})\n",
    "    model.train()\n",
    "    \n",
    "    # Early Stopping 적용\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping!! Goog luck!\")\n",
    "        break\n",
    "        \n",
    "    model_save_path = os.path.join(output, f'{config.model}_{config.data_ag_method}_{config.loss}_classification_model_{i+1}.pt')\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    wandb.save(os.path.join(output, f'{config.model}_{config.data_ag_method}_{config.loss}_classification_model_{i+1}.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33caec83",
   "metadata": {},
   "source": [
    "# 8. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "pred = []\n",
    "probabilities = []  # 예측 확률값을 저장할 리스트\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids_batch, token_type_id_batch, attention_masks_batch in tqdm(test_dataloader):\n",
    "        y_pred = model(input_ids_batch.to(device),token_type_ids = token_type_id_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        pred.extend(predicted.tolist())\n",
    "        \n",
    "        probabilities.extend(F.softmax(y_pred, dim=1).tolist())\n",
    "\n",
    "test_df['predicted_label'] = pred\n",
    "test_df['probability_negative'] = [prob[0] for prob in probabilities]\n",
    "test_df['probability_positive'] = [prob[1] for prob in probabilities]\n",
    "\n",
    "mismatched_samples = test_df[test_df['label'] != test_df['predicted_label']]\n",
    "print(mismatched_samples[['document', 'label', 'predicted_label', 'probability_negative', 'probability_positive']])\n",
    "        \n",
    "accuracy = accuracy_score(pred, test_df['label'])\n",
    "print(accuracy_score(pred, test_df['label']))\n",
    "classification_report_str = classification_report(test_df['label'], pred, target_names=['negative', 'positive'])\n",
    "print(classification_report(test_df['label'], pred, target_names=['negative', 'positive']))\n",
    "\n",
    "cm = confusion_matrix(test_df['label'], pred)\n",
    "sns.heatmap(cm, annot = True, cmap='coolwarm', xticklabels=['negative', 'positive'], yticklabels=['negative', 'positive'], fmt='d')\n",
    "plt.show()\n",
    "\n",
    "wandb.log({\"test_accuracy\": accuracy})\n",
    "wandb.log({\"classification_report\": classification_report_str})\n",
    "wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(probs=None,\n",
    "                                                           y_true=test_df['label'],\n",
    "                                                           preds=pred,\n",
    "                                                           class_names=['negative', 'positive'])})\n",
    "\n",
    "wandb.finish() #훈련과 평가가 다 끝났을때 해주는 것이 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94176d0",
   "metadata": {},
   "source": [
    "## 8-1. 틀린 분류 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628bd338",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check_df = test_df.copy() #테스트 데이터프레임 복사\n",
    "pred1 = pd.Series(pred, name=\"pred\") #예측된 레이블 시리즈로 만들고 이름 붙혀주기\n",
    "test_check_df = pd.concat([test_check_df, pred1], axis = 1) #두개를 하나의 데이터프레임으로 합치기\n",
    "\n",
    "diff_indices = test_check_df[test_check_df['label'] != test_check_df['pred']].index #답변이 틀린것들의 index를 뽑을 수 있음\n",
    "diff_indices\n",
    "#test_check_df.to_csv(data_path +'test_check_df.csv') #혹시나 저장하려면 data_path 잘 확인해서 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df840bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_check(index): #diff_indices에서 나온 index를 하나씩 넣으면 실제답변, 예측답변, document를 볼 수 있음 / 하나씩 확인\n",
    "    print('실제 답변:{}, 예측 답변:{}'.format(test_check_df['label'][index],test_check_df['pred'][index]))\n",
    "    print('document: \\n',test_check_df['document'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d218df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c80414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_all_check(diff_indices):\n",
    "    for i in diff_indices:\n",
    "        print(i)\n",
    "        print('실제 답변:{}, 예측 답변:{}'.format(test_check_df['label'][i],test_check_df['pred'][i]))\n",
    "        print('document: \\n',test_check_df['document'][i])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_all_check(diff_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b211b34",
   "metadata": {},
   "source": [
    "## 8-2. Perplexity (PPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59065a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(aug_data: pd.Dataframe, aug_back_data: pd.Dataframe) -> float:\n",
    "    \"\"\"\n",
    "    Multi 증강 방법에서 perplexity 비교\n",
    "    \n",
    "    Input:\n",
    "        aug_data (pd.Dataframe): 하나의 증강법을 적용한 데이터프레임\n",
    "        aug_back_data (pd.Dataframe): Multi 증강법을 적용한 데이터프레임\n",
    "        \n",
    "    Output:\n",
    "        aug_ppl_avg (float) : 하나의 증강법의 데이터들에 대한 perplexity score\n",
    "        aug_back_ppl_avg (float) : Multi 증강법의 데이터들에 대한 perplexity score\n",
    "    \"\"\"\n",
    "    aug_ppl = []\n",
    "    aug_back_ppl = []\n",
    "    \n",
    "    aug_list = aug_data['document'].tolist()\n",
    "    aug_back_list = aug_back_data['document'].tolist()\n",
    "    \n",
    "    for text in aug_list:\n",
    "        predictions = [text]\n",
    "        perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "        results = perplexity.compute(predictions=predictions, model_id='skt/kogpt2-base-v2', add_start_token=False)\n",
    "        aug_ppl.append(results['perplexities'][0])\n",
    "\n",
    "    for text in aug_back_list:\n",
    "        predictions = [text]\n",
    "        perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "        results = perplexity.compute(predictions=predictions, model_id='skt/kogpt2-base-v2', add_start_token=False)\n",
    "        aug_back_ppl.append(results['perplexities'][0])\n",
    "    \n",
    "    aug_ppl_avg = sum(aug_ppl) / len(aug_ppl)\n",
    "    aug_back_ppl_avg = sum(aug_back_ppl) / len(aug_back_ppl)\n",
    "    \n",
    "    return aug_ppl_avg, aug_back_ppl_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_ppl_avg, mix_back_ppl_avg = perplexity(augmented_df_1, augmented_df)\n",
    "print(\"mix_ppl_avg:\", mix_ppl_avg)\n",
    "print(\"mix_back_ppl_avg:\", mix_back_ppl_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
